<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>mlbio</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell markdown">
<h4 id="tom-genlis--arnaud-baradat---scia-2024">Tom Genlis &amp; Arnaud
Baradat - SCIA 2024</h4>
<h1
id="kaggle-competition-ml-olympiad---genome-sequences-classification">Kaggle
Competition: ML Olympiad - Genome Sequences Classification</h1>
<h1 id="introduction">Introduction</h1>
<p>In the realm of genomics, understanding the distinct genetic
variations among different species is crucial. One application of this
understanding is the classification of DNA sequences, which can, for
instance, differentiate between human and gorilla DNA. This analysis
aims to investigate various machine learning approaches to classify
given DNA sequences into one of these two categories, leveraging a
dataset provided in the Kaggle competition, <a
href="https://www.kaggle.com/competitions/ml-olympiad-gdscuiz-and-tfugagadir/data">ML
Olympiad - Genome Sequences Classification</a>.</p>
<p>The dataset comprises sequences of DNA, each labeled as either
'human' or 'gorilla'. The primary goal is to build and compare models
that can accurately predict these labels based on the DNA sequences.
Five different approaches will be explored:</p>
<ul>
<li>Convolutional Neural Networks (CNN)</li>
<li>Long Short-Term Memory networks (LSTM)</li>
<li>Pre-trained BERT model specialized for genomic sequences</li>
<li>Random Forest</li>
<li>Multi Layer Perceptron (MLP)</li>
</ul>
<p>Through this analysis, we aim to discern which model or combination
of models yields the highest accuracy in classifying the DNA sequences.
Additionally, we will look into potential improvements and optimizations
to enhance the performance of the chosen models.</p>
</div>
<div class="cell markdown">
<hr />
</div>
<div class="cell code">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pandas</span></code></pre></div>
</div>
<section id="dataset" class="cell markdown">
<h1>Dataset</h1>
<p>The dataset provided in the <a
href="https://www.kaggle.com/competitions/ml-olympiad-gdscuiz-and-tfugagadir/data">ML
Olympiad - Genome Sequences Classification competition</a> consists of
DNA sequences labeled as either 'human' or 'gorilla'. In this section,
we'll explore the dataset to understand its structure, size, and the
distribution of labels. We'll also conduct some preliminary analysis to
identify any potential challenges or considerations for the subsequent
modeling steps.</p>
</section>
<div class="cell code" data-execution_count="1"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T16:19:26.567090Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T16:19:26.566715Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T16:20:20.019700Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T16:20:20.018756Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T16:19:26.567057Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&quot;./data/train.csv&quot;</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first few rows of the dataset</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>data.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="1">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>id</th>
      <th>genome_sequence</th>
      <th>species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>11408003</td>
      <td>ccacatcccctccagcacctgttgtttcctgactttttaatgattg...</td>
      <td>Gorilla_gorilla</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>18639873</td>
      <td>tgtttacttgccaatctttgtttagctgtcagagtggcttgctaaa...</td>
      <td>Gorilla_gorilla</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>9869298</td>
      <td>tctgtgaagaaagacattggtagcttgatggggatgacattgaatc...</td>
      <td>Homo_sapiens</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>10762804</td>
      <td>ttgtgagaattacgtgagatgatagatttagggactatagaatagt...</td>
      <td>Gorilla_gorilla</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>13724428</td>
      <td>gcaaaaaataagttgataagttgattgatatgttattagcttaatt...</td>
      <td>Gorilla_gorilla</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown">
<p>Let's lower the row count to speed up the analysis, as we won't use
that much data anyway.</p>
</div>
<div class="cell code" data-execution_count="2"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T16:20:20.021951Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T16:20:20.021632Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T16:20:21.345840Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T16:20:21.344817Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T16:20:20.021920Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.sample(n<span class="op">=</span><span class="dv">1000000</span>)</span></code></pre></div>
</div>
<section id="dataset-structure" class="cell markdown">
<h2>Dataset Structure</h2>
<p>Let's start by examining the structure of the dataset including the
number of samples, features, and the distribution of labels.</p>
</section>
<div class="cell code" data-execution_count="7"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T14:27:21.106036Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T14:27:21.105710Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T14:27:21.182208Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T14:27:21.180844Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T14:27:21.106009Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Getting the shape of the dataset</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>dataset_shape <span class="op">=</span> data.shape</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;The dataset contains </span><span class="sc">{</span>dataset_shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> samples and </span><span class="sc">{</span>dataset_shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> columns.&quot;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Checking the distribution of labels</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>label_distribution <span class="op">=</span> data[<span class="st">&quot;species&quot;</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>label_distribution</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The dataset contains 1000000 samples and 4 columns.
</code></pre>
</div>
<div class="output execute_result" data-execution_count="7">
<pre><code>species
Homo_sapiens       0.500214
Gorilla_gorilla    0.499786
Name: proportion, dtype: float64</code></pre>
</div>
</div>
<section id="preliminary-analysis" class="cell markdown">
<h2>Preliminary Analysis</h2>
<p>We'll conduct some preliminary analysis to better understand the
characteristics of the DNA sequences. This includes examining the length
of the sequences, the distribution of nucleotide bases (A, C, G, T), and
any missing or anomalous values.</p>
</section>
<div class="cell code" data-execution_count="9"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T14:27:42.706241Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T14:27:42.705896Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T14:27:42.960585Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T14:27:42.959597Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T14:27:42.706216Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Checking for missing values</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>missing_values <span class="op">=</span> data.isnull().<span class="bu">sum</span>()</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>missing_values</span></code></pre></div>
<div class="output execute_result" data-execution_count="9">
<pre><code>Unnamed: 0         0
id                 0
genome_sequence    0
species            0
dtype: int64</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="10"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T14:27:43.776299Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T14:27:43.775962Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T14:27:44.223899Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T14:27:44.222989Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T14:27:43.776274Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Exploring the length of DNA sequences</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>sequence_lengths <span class="op">=</span> data[<span class="st">&quot;genome_sequence&quot;</span>].<span class="bu">apply</span>(<span class="bu">len</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>sequence_lengths.describe()</span></code></pre></div>
<div class="output execute_result" data-execution_count="10">
<pre><code>count    1000000.000000
mean          79.999977
std            0.023000
min           57.000000
25%           80.000000
50%           80.000000
75%           80.000000
max           80.000000
Name: genome_sequence, dtype: float64</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="17"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T14:43:12.460359Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T14:43:12.460023Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T14:43:14.160768Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T14:43:14.159709Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T14:43:12.460332Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to count the occurrences of each nucleotide in a sequence</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenating all sequences into a single string</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>all_sequences <span class="op">=</span> <span class="st">&quot;&quot;</span>.join(data[<span class="st">&quot;genome_sequence&quot;</span>])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Counting the occurrences of each nucleotide</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>base_counts <span class="op">=</span> pd.Series(</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;a&quot;</span>: all_sequences.count(<span class="st">&quot;a&quot;</span>),</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;c&quot;</span>: all_sequences.count(<span class="st">&quot;c&quot;</span>),</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;g&quot;</span>: all_sequences.count(<span class="st">&quot;g&quot;</span>),</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;t&quot;</span>: all_sequences.count(<span class="st">&quot;t&quot;</span>),</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Displaying the counts</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>base_counts</span></code></pre></div>
<div class="output execute_result" data-execution_count="17">
<pre><code>a    23912760
c    16068705
g    15967064
t    24051448
dtype: int64</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The preliminary analysis provides insight into the basic
characteristics of the DNA sequences in the dataset. The findings from
this section will inform the choice and configuration of machine
learning models in the subsequent steps of this analysis.</p>
</div>
<div class="cell markdown">
<hr />
</div>
<section id="approaches" class="cell markdown">
<h1>Approaches</h1>
<p>In this section, we explore various machine learning approaches to
classify the DNA sequences as either human or gorilla. The models chosen
for this analysis span a range of complexities and methodologies, from
traditional machine learning to deep learning architectures. The
objective is to compare the performance and insights gleaned from each
model, thereby identifying the most effective strategy for this
classification task. The approaches considered include Convolutional
Neural Networks (CNN), a genomic sequences pre-trained BERT model, Long
Short-Term Memory networks (LSTM), Random Forest, and Multi Layer
Perceptron (MLP).</p>
</section>
<section id="convolutional-neural-networks-cnn" class="cell markdown">
<h2>Convolutional Neural Networks (CNN)</h2>
<p>Convolutional Neural Networks (CNN) are particularly adept at
identifying patterns in spatial or temporal data, making them a suitable
choice for sequence data like DNA sequences. The convolution layers can
detect motifs in the DNA sequences which can be crucial for accurate
classification.</p>
<p>We had two approaches for the CNN model:</p>
<ul>
<li>the first one reshaped the one-hot encoded DNA sequences into a 2D
image and used a CNN model to classify the images</li>
<li>the second one used a 1D CNN model to classify the DNA sequences
directly</li>
</ul>
</section>
<div class="cell code">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pandas tensorflow numpy matplotlib scikit<span class="op">-</span>learn</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder, LabelEncoder</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="output stream stderr">
<pre><code>/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5
  warnings.warn(f&quot;A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}&quot;
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> pd.read_csv(</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;./data/train.csv&quot;</span>,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    usecols<span class="op">=</span>[<span class="st">&quot;genome_sequence&quot;</span>, <span class="st">&quot;species&quot;</span>],</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> train_data.sample(n<span class="op">=</span><span class="dv">500000</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>species_counts <span class="op">=</span> train_data[<span class="st">&quot;species&quot;</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(species_counts)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>species
Gorilla_gorilla    0.500042
Homo_sapiens       0.499958
Name: proportion, dtype: float64
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the model</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.models.Sequential(</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Conv2D(</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>            filters<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>            input_shape<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">4</span>),</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="st">&quot;same&quot;</span>,</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>            kernel_regularizer<span class="op">=</span>tf.keras.regularizers.l2(<span class="fl">0.01</span>),</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.BatchNormalization(),</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.MaxPooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>),</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Conv2D(</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>            filters<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>,</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="st">&quot;same&quot;</span>,</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>            kernel_regularizer<span class="op">=</span>tf.keras.regularizers.l2(<span class="fl">0.01</span>),</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.BatchNormalization(),</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.MaxPooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>),</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Conv2D(</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>            filters<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>,</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="st">&quot;same&quot;</span>,</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>            kernel_regularizer<span class="op">=</span>tf.keras.regularizers.l2(<span class="fl">0.01</span>),</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.BatchNormalization(),</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.MaxPooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>),</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Flatten(),</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Dense(</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>            <span class="dv">512</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>, kernel_regularizer<span class="op">=</span>tf.keras.regularizers.l2(<span class="fl">0.01</span>)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Dense(</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>            <span class="dv">2</span>, activation<span class="op">=</span><span class="st">&quot;softmax&quot;</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>        ),  <span class="co"># Assuming 2 classes, adjust if necessary</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>model.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 9, 9, 64)          2368      
                                                                 
 batch_normalization (BatchN  (None, 9, 9, 64)         256       
 ormalization)                                                   
                                                                 
 max_pooling2d (MaxPooling2D  (None, 5, 5, 64)         0         
 )                                                               
                                                                 
 dropout (Dropout)           (None, 5, 5, 64)          0         
                                                                 
 conv2d_1 (Conv2D)           (None, 5, 5, 128)         73856     
                                                                 
 batch_normalization_1 (Batc  (None, 5, 5, 128)        512       
 hNormalization)                                                 
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 3, 3, 128)        0         
 2D)                                                             
                                                                 
 dropout_1 (Dropout)         (None, 3, 3, 128)         0         
                                                                 
 conv2d_2 (Conv2D)           (None, 3, 3, 256)         295168    
                                                                 
 batch_normalization_2 (Batc  (None, 3, 3, 256)        1024      
 hNormalization)                                                 
                                                                 
 max_pooling2d_2 (MaxPooling  (None, 2, 2, 256)        0         
 2D)                                                             
                                                                 
 dropout_2 (Dropout)         (None, 2, 2, 256)         0         
                                                                 
 flatten (Flatten)           (None, 1024)              0         
                                                                 
 dense (Dense)               (None, 512)               524800    
                                                                 
 dropout_3 (Dropout)         (None, 512)               0         
                                                                 
 dense_1 (Dense)             (None, 2)                 1026      
                                                                 
=================================================================
Total params: 899,010
Trainable params: 898,114
Non-trainable params: 896
_________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_dataset(data, test<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> one_hot_encode(sequence):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        mapping <span class="op">=</span> {</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;a&quot;</span>: [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;c&quot;</span>: [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;g&quot;</span>: [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;t&quot;</span>: [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [mapping[char] <span class="cf">for</span> char <span class="kw">in</span> sequence]</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    encoded_seqs <span class="op">=</span> data[<span class="st">&quot;genome_sequence&quot;</span>].<span class="bu">apply</span>(one_hot_encode)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    padded_seqs <span class="op">=</span> tf.keras.preprocessing.sequence.pad_sequences(</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        encoded_seqs, padding<span class="op">=</span><span class="st">&quot;post&quot;</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> test:</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encode the species labels into numerical values</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        encoded_labels <span class="op">=</span> label_encoder.fit_transform(data[<span class="st">&quot;species&quot;</span>])</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        encoded_labels <span class="op">=</span> tf.keras.utils.to_categorical(encoded_labels, num_classes<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    num_samples <span class="op">=</span> <span class="bu">len</span>(padded_seqs)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    reshaped_data <span class="op">=</span> np.zeros(</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        (num_samples, <span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">4</span>)</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># Initialize an empty array of the desired shape</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, sequence <span class="kw">in</span> <span class="bu">enumerate</span>(padded_seqs):</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape the sequence to a 2D 80x4 matrix</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        sequence_2d <span class="op">=</span> sequence.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">4</span>)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pad this matrix with a row of zeros to get a 81x4 matrix</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        padded_sequence <span class="op">=</span> np.vstack((sequence_2d, np.zeros((<span class="dv">1</span>, <span class="dv">4</span>))))</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape this matrix to a 9x9x4 tensor</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        reshaped_sequence <span class="op">=</span> padded_sequence.reshape(<span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">4</span>)</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store the reshaped sequence in the reshaped_data array</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>        reshaped_data[i] <span class="op">=</span> reshaped_sequence</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> test:</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> reshaped_data, encoded_labels</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> reshaped_data</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>x_train_data, y_train_data <span class="op">=</span> preprocess_dataset(train_data)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    x_train_data, y_train_data, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_images(data_piece):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">5</span>))</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Assuming data_piece has shape (9, 9, 4)</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> data_piece[:, :, i]</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        axs[i].imshow(img, cmap<span class="op">=</span><span class="st">&quot;gray&quot;</span>)  <span class="co"># or choose a different colormap if preferred</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        axs[i].axis(<span class="st">&quot;off&quot;</span>)  <span class="co"># to remove the axes for clarity</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        axs[i].set_title(<span class="ss">f&quot;Channel </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>plot_images(x_train[<span class="dv">0</span>])</span></code></pre></div>
<div class="output display_data">
<p><img src="f382a48878b67176079612cf3f6479c6365b52a4.png" /></p>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">&quot;categorical_crossentropy&quot;</span>, optimizer<span class="op">=</span>optimizer, metrics<span class="op">=</span>[<span class="st">&quot;accuracy&quot;</span>]</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Early Stopping and Reduce LR On Plateau callbacks</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>early_stopping <span class="op">=</span> tf.keras.callbacks.EarlyStopping(</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">&quot;val_loss&quot;</span>, patience<span class="op">=</span><span class="dv">5</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>, verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>reduce_lr <span class="op">=</span> tf.keras.callbacks.ReduceLROnPlateau(</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">&quot;val_loss&quot;</span>, factor<span class="op">=</span><span class="fl">0.2</span>, patience<span class="op">=</span><span class="dv">3</span>, min_lr<span class="op">=</span><span class="fl">0.00001</span>, verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with the callbacks</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    x_train,</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    y_train,</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(x_test, y_test),</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">25</span>,</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stopping, reduce_lr],</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/25
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>2023-10-06 15:36:37.673959: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>3125/3125 [==============================] - 29s 6ms/step - loss: 1.0012 - accuracy: 0.5022 - val_loss: 0.6931 - val_accuracy: 0.5006 - lr: 0.0010
Epoch 2/25
3125/3125 [==============================] - 22s 7ms/step - loss: 0.6932 - accuracy: 0.4984 - val_loss: 0.6932 - val_accuracy: 0.4994 - lr: 0.0010
Epoch 3/25
3125/3125 [==============================] - 18s 6ms/step - loss: 0.6932 - accuracy: 0.4994 - val_loss: 0.6932 - val_accuracy: 0.4994 - lr: 0.0010
Epoch 4/25
3119/3125 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5001
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
3125/3125 [==============================] - 18s 6ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.5006 - lr: 0.0010
Epoch 5/25
3125/3125 [==============================] - 18s 6ms/step - loss: 0.6931 - accuracy: 0.5012 - val_loss: 0.6931 - val_accuracy: 0.5006 - lr: 2.0000e-04
Epoch 6/25
3125/3125 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4997Restoring model weights from the end of the best epoch: 1.
3125/3125 [==============================] - 18s 6ms/step - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.4994 - lr: 2.0000e-04
Epoch 6: early stopping
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the training and validation accuracy</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">&quot;accuracy&quot;</span>], label<span class="op">=</span><span class="st">&quot;Training Accuracy&quot;</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">&quot;val_accuracy&quot;</span>], label<span class="op">=</span><span class="st">&quot;Validation Accuracy&quot;</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Training and Validation Accuracy&quot;</span>)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the training and validation loss</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">&quot;loss&quot;</span>], label<span class="op">=</span><span class="st">&quot;Training Loss&quot;</span>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">&quot;val_loss&quot;</span>], label<span class="op">=</span><span class="st">&quot;Validation Loss&quot;</span>)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Training and Validation Loss&quot;</span>)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img src="cdefc929470f17eb0633af7f4bc71af95ae1f09d.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>This approch led to an accuracy fluctuating around 50% which is not
really good considered this is random guessing. However, as we will see
thourought the analysis, and also in the best scores of the competition,
the task is very hard and no model was able to get a good accuracy.</p>
</div>
<div class="cell markdown">
<p>We had another approach of CNNs which used 1D convolutions. Let's see
how it performs.</p>
</div>
<div class="cell code">
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pandas numpy scikit<span class="op">-</span>learn matplotlib torch torchvision torchaudio tqdm</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>random_state <span class="op">=</span> <span class="dv">42</span></span></code></pre></div>
<div class="output stream stderr">
<pre><code>/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5
  warnings.warn(f&quot;A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}&quot;
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;./data/train.csv&quot;</span>, usecols<span class="op">=</span>[<span class="st">&quot;id&quot;</span>, <span class="st">&quot;genome_sequence&quot;</span>, <span class="st">&quot;species&quot;</span>]</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Removes the index column of the csv</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Shape of the dataset: &quot;</span>, df.shape)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Shape of the dataset:  (19800000, 3)
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<div class="output display_data">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>genome_sequence</th>
      <th>species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>11408003</td>
      <td>ccacatcccctccagcacctgttgtttcctgactttttaatgattg...</td>
      <td>Gorilla_gorilla</td>
    </tr>
    <tr>
      <th>1</th>
      <td>18639873</td>
      <td>tgtttacttgccaatctttgtttagctgtcagagtggcttgctaaa...</td>
      <td>Gorilla_gorilla</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9869298</td>
      <td>tctgtgaagaaagacattggtagcttgatggggatgacattgaatc...</td>
      <td>Homo_sapiens</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10762804</td>
      <td>ttgtgagaattacgtgagatgatagatttagggactatagaatagt...</td>
      <td>Gorilla_gorilla</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13724428</td>
      <td>gcaaaaaataagttgataagttgattgatatgttattagcttaatt...</td>
      <td>Gorilla_gorilla</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check that there are no missing values</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>df.isnull().<span class="bu">sum</span>()</span></code></pre></div>
<div class="output display_data">
<pre><code>id                 0
genome_sequence    0
species            0
dtype: int64</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb38"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the balancing of the target</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>df.species.value_counts()</span></code></pre></div>
<div class="output display_data">
<pre><code>species
Homo_sapiens       9900585
Gorilla_gorilla    9899415
Name: count, dtype: int64</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the average length of the genome sequences</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>df.genome_sequence.<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x)).value_counts()</span></code></pre></div>
<div class="output display_data">
<pre><code>genome_sequence
80    19799984
37           2
39           1
2            1
30           1
25           1
16           1
3            1
15           1
57           1
48           1
32           1
31           1
43           1
60           1
50           1
Name: count, dtype: int64</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's remove the few that aren't of the 80 long</p>
</div>
<div class="cell code">
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove the &lt;20 genome sequences that are not 80 long</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[df.genome_sequence.<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x)) <span class="op">==</span> <span class="dv">80</span>]</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>df.shape</span></code></pre></div>
<div class="output display_data">
<pre><code>(19799984, 3)</code></pre>
</div>
</div>
<section id="data-preparation" class="cell markdown">
<h2>Data Preparation</h2>
</section>
<div class="cell code">
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a label column that change the species name into a number</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>labels_dict <span class="op">=</span> {species: i <span class="cf">for</span> i, species <span class="kw">in</span> <span class="bu">enumerate</span>(df.species.unique())}</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&quot;label&quot;</span>] <span class="op">=</span> df.species.<span class="bu">map</span>(labels_dict)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<div class="output display_data">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>genome_sequence</th>
      <th>species</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>11408003</td>
      <td>ccacatcccctccagcacctgttgtttcctgactttttaatgattg...</td>
      <td>Gorilla_gorilla</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>18639873</td>
      <td>tgtttacttgccaatctttgtttagctgtcagagtggcttgctaaa...</td>
      <td>Gorilla_gorilla</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9869298</td>
      <td>tctgtgaagaaagacattggtagcttgatggggatgacattgaatc...</td>
      <td>Homo_sapiens</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10762804</td>
      <td>ttgtgagaattacgtgagatgatagatttagggactatagaatagt...</td>
      <td>Gorilla_gorilla</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13724428</td>
      <td>gcaaaaaataagttgataagttgattgatatgttattagcttaatt...</td>
      <td>Gorilla_gorilla</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb45"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to one hot encode a DNA sequence</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot_encode_dna(seq):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>                <span class="dv">1</span> <span class="cf">if</span> c <span class="op">==</span> <span class="st">&quot;A&quot;</span> <span class="cf">else</span> <span class="dv">0</span>,</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>                <span class="dv">1</span> <span class="cf">if</span> c <span class="op">==</span> <span class="st">&quot;C&quot;</span> <span class="cf">else</span> <span class="dv">0</span>,</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>                <span class="dv">1</span> <span class="cf">if</span> c <span class="op">==</span> <span class="st">&quot;G&quot;</span> <span class="cf">else</span> <span class="dv">0</span>,</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>                <span class="dv">1</span> <span class="cf">if</span> c <span class="op">==</span> <span class="st">&quot;T&quot;</span> <span class="cf">else</span> <span class="dv">0</span>,</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> c <span class="kw">in</span> seq.upper()</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop unnecessary columns</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">&quot;id&quot;</span>, <span class="st">&quot;species&quot;</span>])</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<div class="output display_data">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>genome_sequence</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ccacatcccctccagcacctgttgtttcctgactttttaatgattg...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>tgtttacttgccaatctttgtttagctgtcagagtggcttgctaaa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>tctgtgaagaaagacattggtagcttgatggggatgacattgaatc...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ttgtgagaattacgtgagatgatagatttagggactatagaatagt...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>gcaaaaaataagttgataagttgattgatatgttattagcttaatt...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown">
<p>Let's subsample the dataset to have a smaller dataset to work
with.</p>
</div>
<div class="cell code">
<div class="sourceCode" id="cb47"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into train and test</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>df_train, df_test <span class="op">=</span> train_test_split(</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    df.sample(n<span class="op">=</span><span class="dv">100000</span>), test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span>random_state</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Shape of the train dataset: &quot;</span>, df_train.shape)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Shape of the test dataset: &quot;</span>, df_test.shape)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Shape of the train dataset:  (80000, 2)
Shape of the test dataset:  (20000, 2)
</code></pre>
</div>
</div>
<section id="creating-a-torch-dataset" class="cell markdown">
<h3>Creating a torch dataset</h3>
</section>
<div class="cell code">
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a custom dataset for CNN</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GenomeDatasetCNN(Dataset):</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, X, y, transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.X <span class="op">=</span> X</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> y</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> (</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>            transforms.Compose(</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>                    transforms.ToTensor(),</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>                ]</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> transform <span class="kw">is</span> <span class="va">None</span></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> transform</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.X)</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="va">self</span>.y[idx]</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>        dna <span class="op">=</span> <span class="va">self</span>.X[idx]</span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>        dna <span class="op">=</span> one_hot_encode_dna(dna)</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>        dna <span class="op">=</span> torch.from_numpy(dna)</span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>        dna <span class="op">=</span> torch.reshape(dna, (dna.shape[<span class="dv">1</span>], dna.shape[<span class="dv">0</span>]))</span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convert all to float</span></span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a>        dna <span class="op">=</span> dna.<span class="bu">float</span>()</span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> torch.tensor(label).<span class="bu">float</span>()</span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> dna, label</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb50"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_one_hot_encoded_dna(genome_seq):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the one hot encoded DNA sequence</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">2</span>))</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    plt.imshow(genome_seq, cmap<span class="op">=</span><span class="st">&quot;hot&quot;</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    plt.xticks(<span class="bu">range</span>(<span class="dv">0</span>, genome_seq.shape[<span class="dv">0</span>]))</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    plt.yticks(<span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">4</span>), [<span class="st">&quot;A&quot;</span>, <span class="st">&quot;C&quot;</span>, <span class="st">&quot;G&quot;</span>, <span class="st">&quot;T&quot;</span>])</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>test_print <span class="op">=</span> one_hot_encode_dna(df_train.iloc[<span class="dv">0</span>].genome_sequence).transpose(<span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>print_one_hot_encoded_dna(test_print)</span></code></pre></div>
<div class="output display_data">
<p><img src="cca0a39cf5879466658a38a578ab47cb4e00ea8e.png" /></p>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> GenomeDatasetCNN(</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    df_train[<span class="st">&quot;genome_sequence&quot;</span>].to_numpy(), df_train[<span class="st">&quot;label&quot;</span>].to_numpy()</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co"># print an example of the dataset</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>dna, label <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;DNA shape: &quot;</span>, dna.shape)</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Label shape: &quot;</span>, label.shape)</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Label: &quot;</span>, label)</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;DNA: &quot;</span>, dna[<span class="dv">0</span>])</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>print_one_hot_encoded_dna(dna[<span class="dv">0</span>])  <span class="co"># Print the first DNA sequence of the batch</span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Make our test and train datasets and data loaders</span></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> GenomeDatasetCNN(</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>    df_train[<span class="st">&quot;genome_sequence&quot;</span>].to_numpy(), df_train[<span class="st">&quot;label&quot;</span>].to_numpy()</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> GenomeDatasetCNN(</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>    df_test[<span class="st">&quot;genome_sequence&quot;</span>].to_numpy(), df_test[<span class="st">&quot;label&quot;</span>].to_numpy()</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>, drop_last<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span><span class="va">True</span></span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>DNA shape:  torch.Size([32, 4, 80])
Label shape:  torch.Size([32])
Label:  tensor([1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.])
DNA:  tensor([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
         0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
         0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,
         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,
         0., 0., 0., 1., 0., 0., 0., 1.],
        [0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,
         0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
         0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
         0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
         0., 1., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
         0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
         0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.,
         0., 1., 0., 0., 0., 0., 0., 1.],
        [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
         1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.,
         0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
         1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,
         0., 0., 1., 0., 0., 0., 1., 0.]])
</code></pre>
</div>
<div class="output display_data">
<p><img src="ae4b7f9683ef1193354cd4acd48afaf8d4f700f3.png" /></p>
</div>
</div>
<section id="1d-cnn" class="cell markdown">
<h3>1D CNN</h3>
</section>
<div class="cell code">
<div class="sourceCode" id="cb53"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN(nn.Module):</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CNN, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="co">        # In channel: 4</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="co">        # Use 3 layers of Conv1D with kernel size (2, 3, 3) and stride (0, 1, 1), and padding (0, 1, 1)</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="co">        The in channels are: (4, 16, 32)</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="co">        The out channels are: (16, 32, 64)</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a><span class="co">        # Use 1 layer of MaxPool1D with kernel size (2) and stride (2)</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a><span class="co">        # Use 2 layers of Linear with 128 and 64 neurons</span></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a><span class="co">        # Use 1 layer of Linear with 1 neuron</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv1d(<span class="dv">4</span>, <span class="dv">16</span>, <span class="dv">2</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv1d(<span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv1d(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool1d(<span class="dv">2</span>)</span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">19</span> <span class="op">*</span> <span class="dv">64</span>, <span class="dv">64</span>)</span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">1</span>)</span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv2(x)))</span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv3(x)))</span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.sigmoid(<span class="va">self</span>.fc3(x))</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CNN()</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb54"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    model, optimizer, criterion, dataloader, eval_dataloader, epochs<span class="op">=</span><span class="dv">10</span>, device<span class="op">=</span><span class="st">&quot;cpu&quot;</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>    accuracies <span class="op">=</span> []</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_idx, (genome_seq, label) <span class="kw">in</span> tqdm(</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>            <span class="bu">enumerate</span>(dataloader), total<span class="op">=</span><span class="bu">len</span>(dataloader)</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>        ):</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>            genome_seq <span class="op">=</span> genome_seq.to(device)</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>            label <span class="op">=</span> label.to(device)</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> model(genome_seq)</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(output.view(<span class="op">-</span><span class="dv">1</span>), label)</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>            train_loss <span class="op">+=</span> loss.item()</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;Train loss: &quot;</span>, train_loss <span class="op">/</span> <span class="bu">len</span>(dataloader))</span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>        losses.append(train_loss <span class="op">/</span> <span class="bu">len</span>(dataloader))</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>        y_true <span class="op">=</span> []</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> []</span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_idx, (genome_seq, label) <span class="kw">in</span> <span class="bu">enumerate</span>(eval_dataloader):</span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> model(genome_seq)</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>            y_true.extend(label.numpy())</span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>            y_pred.extend(output.argmax(axis<span class="op">=</span><span class="dv">1</span>).cpu().numpy())</span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>        accuracies.append(accuracy_score(y_true, y_pred))</span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model, losses, accuracies</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb55"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.DataParallel(model)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>model, losses, accuracies <span class="op">=</span> train_model(</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>    optimizer,</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>    criterion,</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>    train_dataloader,</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>    test_dataloader,</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device,</span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:14&lt;00:00, 172.45it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6932496196269989
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:13&lt;00:00, 180.30it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931665858268737
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:14&lt;00:00, 167.45it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931670973300934
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:13&lt;00:00, 181.31it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931713379621506
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:14&lt;00:00, 173.03it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931791742086411
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:13&lt;00:00, 178.78it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931776783943177
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:14&lt;00:00, 173.33it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931836221694946
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:13&lt;00:00, 178.79it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.693159950375557
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:13&lt;00:00, 179.90it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931603684663773
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:14&lt;00:00, 171.06it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.693187800192833
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:13&lt;00:00, 180.71it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931726255893708
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:14&lt;00:00, 172.14it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931788826227188
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:14&lt;00:00, 178.41it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931809585809707
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:14&lt;00:00, 172.93it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931740037441254
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [00:14&lt;00:00, 175.27it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Train loss:  0.6931683916330338
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb86"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the training and validation accuracy</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>plt.plot(accuracies, label<span class="op">=</span><span class="st">&quot;Val Accuracy&quot;</span>)</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Validation Accuracy&quot;</span>)</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the training and validation loss</span></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>plt.plot(losses, label<span class="op">=</span><span class="st">&quot;Training Loss&quot;</span>)</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Training Loss&quot;</span>)</span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img src="af98c1c76fdf3f31489cd5b10a014f1e848bce5b.png" /></p>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb87"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute accuracy on the test set</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_accuracy(model, dataloader):</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> []</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> []</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_idx, (genome_seq, label) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(genome_seq)</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>        y_true.extend(label.numpy())</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>        y_pred.extend(output.argmax(axis<span class="op">=</span><span class="dv">1</span>).cpu().numpy())</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> accuracy_score(y_true, y_pred)</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Accuracy on the test set: &quot;</span>, compute_accuracy(model, test_dataloader))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Accuracy on the test set:  0.50255
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The 1D conv model seem to be adapting better to the data. However the
stillness of the training's accuracy shows how hard is the network
struggling.</p>
</div>
<div class="cell markdown">
<hr />
</div>
<section id="long-short-term-memory-networks-lstm"
class="cell markdown">
<h2>Long Short-Term Memory Networks (LSTM)</h2>
<p>Long Short-Term Memory networks (LSTM) are a type of recurrent neural
network capable of learning long-term dependencies in data. Given the
sequential nature of DNA, LSTMs can be employed to capture the inherent
dependencies between nucleotides over varying sequence lengths.</p>
</section>
<div class="cell code">
<div class="sourceCode" id="cb89"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pandas tensorflow scikit<span class="op">-</span>learn matplotlib numpy tqdm</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb90"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder, LabelEncoder</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.sequence <span class="im">import</span> pad_sequences</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb91"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> pd.read_csv(<span class="st">&quot;./data/train.csv&quot;</span>, usecols<span class="op">=</span>[<span class="st">&quot;genome_sequence&quot;</span>, <span class="st">&quot;species&quot;</span>])</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> train_data.sample(n<span class="op">=</span><span class="dv">500000</span>)</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>species_counts <span class="op">=</span> train_data[<span class="st">&quot;species&quot;</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(species_counts)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>species
Gorilla_gorilla    0.500052
Homo_sapiens       0.499948
Name: proportion, dtype: float64
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb93"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>train_data.head()</span></code></pre></div>
<div class="output display_data">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>genome_sequence</th>
      <th>species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7304829</th>
      <td>tcctaccaatttctctcaaaacacacatcctttatcctcttaacac...</td>
      <td>Homo_sapiens</td>
    </tr>
    <tr>
      <th>13708569</th>
      <td>ctactacaaaacaaacagtaagaaattactgatatatgcaacaacc...</td>
      <td>Gorilla_gorilla</td>
    </tr>
    <tr>
      <th>16732947</th>
      <td>ctattattcttttcttttttgagatggattttccctcttgttgccc...</td>
      <td>Homo_sapiens</td>
    </tr>
    <tr>
      <th>18925439</th>
      <td>taaagtgctggaattacaggcatgagccaccgcccccggccaaaat...</td>
      <td>Homo_sapiens</td>
    </tr>
    <tr>
      <th>2509507</th>
      <td>cagcatatattggcaaagttattctggtcagatgatgcacacaatg...</td>
      <td>Gorilla_gorilla</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb94"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>nucleotide_mapping <span class="op">=</span> {<span class="st">&quot;a&quot;</span>: <span class="dv">0</span>, <span class="st">&quot;c&quot;</span>: <span class="dv">1</span>, <span class="st">&quot;g&quot;</span>: <span class="dv">2</span>, <span class="st">&quot;t&quot;</span>: <span class="dv">3</span>}</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>encoded_sequences <span class="op">=</span> train_data[<span class="st">&quot;genome_sequence&quot;</span>].<span class="bu">apply</span>(</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> x: [nucleotide_mapping[n] <span class="cf">for</span> n <span class="kw">in</span> x]</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>train_data[<span class="st">&quot;encoded_sequence&quot;</span>] <span class="op">=</span> encoded_sequences</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Label Encoding</span></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>train_data[<span class="st">&quot;encoded_species&quot;</span>] <span class="op">=</span> label_encoder.fit_transform(train_data[<span class="st">&quot;species&quot;</span>])</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Padding</span></span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>SEQUENCE_LENGTH <span class="op">=</span> <span class="bu">max</span>(train_data[<span class="st">&quot;encoded_sequence&quot;</span>].<span class="bu">apply</span>(<span class="bu">len</span>))</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pad_sequences(</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>    train_data[<span class="st">&quot;encoded_sequence&quot;</span>], maxlen<span class="op">=</span>SEQUENCE_LENGTH, padding<span class="op">=</span><span class="st">&quot;post&quot;</span></span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.keras.utils.to_categorical(train_data[<span class="st">&quot;encoded_species&quot;</span>])</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. LSTM model</span></span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.models.Sequential(</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Embedding(<span class="dv">4</span>, <span class="dv">8</span>, input_length<span class="op">=</span>SEQUENCE_LENGTH),</span>
<span id="cb94-27"><a href="#cb94-27" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.LSTM(<span class="dv">64</span>, return_sequences<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb94-28"><a href="#cb94-28" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.LSTM(<span class="dv">32</span>),</span>
<span id="cb94-29"><a href="#cb94-29" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Dense(y.shape[<span class="dv">1</span>], activation<span class="op">=</span><span class="st">&quot;softmax&quot;</span>),</span>
<span id="cb94-30"><a href="#cb94-30" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb94-31"><a href="#cb94-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-32"><a href="#cb94-32" aria-hidden="true" tabindex="-1"></a>model.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;sequential_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_3 (Embedding)     (None, 80, 8)             32        
                                                                 
 lstm_6 (LSTM)               (None, 80, 64)            18688     
                                                                 
 lstm_7 (LSTM)               (None, 32)                12416     
                                                                 
 dense_2 (Dense)             (None, 2)                 66        
                                                                 
=================================================================
Total params: 31,202
Trainable params: 31,202
Non-trainable params: 0
_________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb96"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_images(data_piece):</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>    fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">5</span>))</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Assuming data_piece has shape (9, 9, 4)</span></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> data_piece[:, :, i]</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>        axs[i].imshow(img, cmap<span class="op">=</span><span class="st">&quot;gray&quot;</span>)  <span class="co"># or choose a different colormap if preferred</span></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>        axs[i].axis(<span class="st">&quot;off&quot;</span>)  <span class="co"># to remove the axes for clarity</span></span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>        axs[i].set_title(<span class="ss">f&quot;Channel </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>plot_images(X_train[<span class="dv">0</span>])</span></code></pre></div>
<div class="output display_data">
<p><img src="6b76a2ee3904e04c6970bdffc6223113da1a46c6.png" /></p>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb97"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">&quot;categorical_crossentropy&quot;</span>, optimizer<span class="op">=</span>optimizer, metrics<span class="op">=</span>[<span class="st">&quot;accuracy&quot;</span>]</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Early Stopping and Reduce LR On Plateau callbacks</span></span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>early_stopping <span class="op">=</span> tf.keras.callbacks.EarlyStopping(</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">&quot;val_loss&quot;</span>, patience<span class="op">=</span><span class="dv">5</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>, verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>reduce_lr <span class="op">=</span> tf.keras.callbacks.ReduceLROnPlateau(</span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">&quot;val_loss&quot;</span>, factor<span class="op">=</span><span class="fl">0.2</span>, patience<span class="op">=</span><span class="dv">3</span>, min_lr<span class="op">=</span><span class="fl">0.00001</span>, verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb98"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with the callbacks</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>    X_train,</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>    y_train,</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_test, y_test),</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stopping, reduce_lr],</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/5
3125/3125 [==============================] - 34s 11ms/step - loss: 0.6931 - accuracy: 0.5036 - val_loss: 0.6930 - val_accuracy: 0.5052 - lr: 0.0010
Epoch 2/5
3125/3125 [==============================] - 33s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6930 - val_accuracy: 0.5052 - lr: 0.0010
Epoch 3/5
3125/3125 [==============================] - 34s 11ms/step - loss: 0.6911 - accuracy: 0.5056 - val_loss: 0.6899 - val_accuracy: 0.5056 - lr: 0.0010
Epoch 4/5
3125/3125 [==============================] - 34s 11ms/step - loss: 0.6898 - accuracy: 0.5047 - val_loss: 0.6899 - val_accuracy: 0.5046 - lr: 0.0010
Epoch 5/5
3125/3125 [==============================] - 35s 11ms/step - loss: 0.6901 - accuracy: 0.5058 - val_loss: 0.6899 - val_accuracy: 0.5043 - lr: 0.0010
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb100"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the training and validation accuracy</span></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">&quot;accuracy&quot;</span>], label<span class="op">=</span><span class="st">&quot;Training Accuracy&quot;</span>)</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">&quot;val_accuracy&quot;</span>], label<span class="op">=</span><span class="st">&quot;Validation Accuracy&quot;</span>)</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Training and Validation Accuracy&quot;</span>)</span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the training and validation loss</span></span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">&quot;loss&quot;</span>], label<span class="op">=</span><span class="st">&quot;Training Loss&quot;</span>)</span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">&quot;val_loss&quot;</span>], label<span class="op">=</span><span class="st">&quot;Validation Loss&quot;</span>)</span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Training and Validation Loss&quot;</span>)</span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img src="b9d7c6885359751ca347993de858907340571d84.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>As you can see the LSTM approach seems to be the best one out of all
the CNNs.</p>
<p>Maybe having a better hardware allowing use to use bigger sizes for
the batch size and the hidden size would have helped generalization.</p>
<p>However, the accuracy is still very low. Let's see if we can improve
it with a pre-trained model using an NLP approach.</p>
</div>
<section id="genomic-sequences-pre-trained-bert" class="cell markdown">
<h2>Genomic Sequences Pre-trained BERT</h2>
<p>The Bidirectional Encoder Representations from Transformers (BERT)
model has shown promise in various NLP tasks. A version of BERT
pre-trained on genomic sequences can potentially capture the contextual
relationships between nucleotides in DNA sequences, thereby aiding in
accurate classification.</p>
</section>
<div class="cell markdown">
<p>Let's start first using the weights of the papers of DNA bert where a
pre-trained model is available.</p>
</div>
<div class="cell markdown">
<p><a
href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb#scrollTo=ikfbFlNHgi8T"
class="uri">https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb#scrollTo=ikfbFlNHgi8T</a></p>
<p><a href="https://huggingface.co/zhihan1996/DNABERT-2-117M"
class="uri">https://huggingface.co/zhihan1996/DNABERT-2-117M</a></p>
</div>
<div class="cell code">
<div class="sourceCode" id="cb101"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install transformers<span class="op">==</span><span class="fl">4.28</span> einops torch torchvision torchaudio transformers tqdm matplotlib scikit<span class="op">-</span>learn pandas numpy</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Requirement already satisfied: transformers==4.28 in /opt/conda/lib/python3.10/site-packages (4.28.0)
Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.7.0)
Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.28) (3.12.2)
Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28) (0.16.4)
Requirement already satisfied: numpy&gt;=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28) (1.23.5)
Requirement already satisfied: packaging&gt;=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28) (21.3)
Requirement already satisfied: pyyaml&gt;=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28) (6.0)
Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28) (2023.6.3)
Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.28) (2.31.0)
Requirement already satisfied: tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28) (0.13.3)
Requirement already satisfied: tqdm&gt;=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28) (4.66.1)
Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.11.0-&gt;transformers==4.28) (2023.9.0)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.11.0-&gt;transformers==4.28) (4.6.3)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging&gt;=20.0-&gt;transformers==4.28) (3.0.9)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.10/site-packages (from requests-&gt;transformers==4.28) (3.1.0)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.10/site-packages (from requests-&gt;transformers==4.28) (3.4)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests-&gt;transformers==4.28) (1.26.15)
Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests-&gt;transformers==4.28) (2023.7.22)
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb103"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModel</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader, RandomSampler, SequentialSampler</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb104"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sections of config</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Defining some key variables that will be used later on in the training</span></span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>MAX_LEN <span class="op">=</span> <span class="dv">90</span></span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>TRAIN_BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a>VALID_BATCH_SIZE <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">1e-03</span></span>
<span id="cb104-9"><a href="#cb104-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-10"><a href="#cb104-10" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(</span>
<span id="cb104-11"><a href="#cb104-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;zhihan1996/DNABERT-2-117M&quot;</span>, trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb104-12"><a href="#cb104-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb105"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomDataset(Dataset):</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dataframe, tokenizer, max_len):</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> dataframe</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.genome_sequence <span class="op">=</span> dataframe.genome_sequence</span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> dataframe.label.to_numpy()</span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_len <span class="op">=</span> max_len</span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.genome_sequence)</span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a>        genome_seq <span class="op">=</span> <span class="bu">str</span>(<span class="va">self</span>.genome_sequence.iloc[index])</span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.tokenizer.encode_plus(</span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a>            genome_seq,</span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a>            <span class="va">None</span>,</span>
<span id="cb105-17"><a href="#cb105-17" aria-hidden="true" tabindex="-1"></a>            add_special_tokens<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb105-18"><a href="#cb105-18" aria-hidden="true" tabindex="-1"></a>            max_length<span class="op">=</span><span class="va">self</span>.max_len,</span>
<span id="cb105-19"><a href="#cb105-19" aria-hidden="true" tabindex="-1"></a>            pad_to_max_length<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb105-20"><a href="#cb105-20" aria-hidden="true" tabindex="-1"></a>            return_token_type_ids<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb105-21"><a href="#cb105-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb105-22"><a href="#cb105-22" aria-hidden="true" tabindex="-1"></a>        ids <span class="op">=</span> inputs[<span class="st">&quot;input_ids&quot;</span>]</span>
<span id="cb105-23"><a href="#cb105-23" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> inputs[<span class="st">&quot;attention_mask&quot;</span>]</span>
<span id="cb105-24"><a href="#cb105-24" aria-hidden="true" tabindex="-1"></a>        token_type_ids <span class="op">=</span> inputs[<span class="st">&quot;token_type_ids&quot;</span>]</span>
<span id="cb105-25"><a href="#cb105-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-26"><a href="#cb105-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb105-27"><a href="#cb105-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;ids&quot;</span>: torch.tensor(ids, dtype<span class="op">=</span>torch.<span class="bu">long</span>),</span>
<span id="cb105-28"><a href="#cb105-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;mask&quot;</span>: torch.tensor(mask, dtype<span class="op">=</span>torch.<span class="bu">long</span>),</span>
<span id="cb105-29"><a href="#cb105-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;token_type_ids&quot;</span>: torch.tensor(token_type_ids, dtype<span class="op">=</span>torch.<span class="bu">long</span>),</span>
<span id="cb105-30"><a href="#cb105-30" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;targets&quot;</span>: torch.tensor(<span class="va">self</span>.targets[index], dtype<span class="op">=</span>torch.<span class="bu">float</span>),</span>
<span id="cb105-31"><a href="#cb105-31" aria-hidden="true" tabindex="-1"></a>        }</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb106"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the dataset and dataloader for the neural network</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;FULL Dataset: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(df.shape))</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;TRAIN Dataset: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(df_train.shape))</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;TEST Dataset: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(df_test.shape))</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>training_set <span class="op">=</span> CustomDataset(df_train, tokenizer, MAX_LEN)</span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>testing_set <span class="op">=</span> CustomDataset(df_test, tokenizer, MAX_LEN)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>FULL Dataset: (19799984, 2)
TRAIN Dataset: (80000, 2)
TEST Dataset: (20000, 2)
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb108"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>train_params <span class="op">=</span> {<span class="st">&quot;batch_size&quot;</span>: TRAIN_BATCH_SIZE, <span class="st">&quot;shuffle&quot;</span>: <span class="va">True</span>, <span class="st">&quot;num_workers&quot;</span>: <span class="dv">0</span>}</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>test_params <span class="op">=</span> {<span class="st">&quot;batch_size&quot;</span>: VALID_BATCH_SIZE, <span class="st">&quot;shuffle&quot;</span>: <span class="va">True</span>, <span class="st">&quot;num_workers&quot;</span>: <span class="dv">0</span>}</span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>training_loader <span class="op">=</span> DataLoader(training_set, <span class="op">**</span>train_params)</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>testing_loader <span class="op">=</span> DataLoader(testing_set, <span class="op">**</span>test_params)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb109"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb110"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model.</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BERTClass(torch.nn.Module):</span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BERTClass, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb110-7"><a href="#cb110-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-8"><a href="#cb110-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l1 <span class="op">=</span> AutoModel.from_pretrained(</span>
<span id="cb110-9"><a href="#cb110-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;zhihan1996/DNABERT-2-117M&quot;</span>, trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb110-10"><a href="#cb110-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb110-11"><a href="#cb110-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l3 <span class="op">=</span> torch.nn.Linear(<span class="dv">768</span>, <span class="dv">1</span>)</span>
<span id="cb110-12"><a href="#cb110-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-13"><a href="#cb110-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, ids, mask, token_type_ids):</span>
<span id="cb110-14"><a href="#cb110-14" aria-hidden="true" tabindex="-1"></a>        _, output_1 <span class="op">=</span> <span class="va">self</span>.l1(</span>
<span id="cb110-15"><a href="#cb110-15" aria-hidden="true" tabindex="-1"></a>            ids, attention_mask<span class="op">=</span>mask, token_type_ids<span class="op">=</span>token_type_ids, return_dict<span class="op">=</span><span class="va">False</span></span>
<span id="cb110-16"><a href="#cb110-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb110-17"><a href="#cb110-17" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.l3(output_1)</span>
<span id="cb110-18"><a href="#cb110-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-19"><a href="#cb110-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb110-20"><a href="#cb110-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-21"><a href="#cb110-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-22"><a href="#cb110-22" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BERTClass()</span>
<span id="cb110-23"><a href="#cb110-23" aria-hidden="true" tabindex="-1"></a>model.to(device)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/81ac6a98387cf94bc283553260f3fa6b88cef2fa/bert_layers.py:125: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: [&#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.decoder.weight&#39;]
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
</div>
<div class="output display_data">
<pre><code>BERTClass(
  (l1): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(4096, 768, padding_idx=0)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertUnpadAttention(
            (self): BertUnpadSelfAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (mlp): BertGatedLinearUnitMLP(
            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)
            (act): GELU(approximate=&#39;none&#39;)
            (wo): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (l3): Linear(in_features=768, out_features=1, bias=True)
)</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb113"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss_fn(outputs, targets):</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.nn.BCEWithLogitsLoss()(outputs, targets)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb114"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(params<span class="op">=</span>model.parameters(), lr<span class="op">=</span>LEARNING_RATE)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb115"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(epoch):</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _, data <span class="kw">in</span> tqdm(<span class="bu">enumerate</span>(training_loader, <span class="dv">0</span>), total<span class="op">=</span><span class="bu">len</span>(training_loader)):</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>        ids <span class="op">=</span> data[<span class="st">&quot;ids&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> data[<span class="st">&quot;mask&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>        token_type_ids <span class="op">=</span> data[<span class="st">&quot;token_type_ids&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> data[<span class="st">&quot;targets&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(ids, mask, token_type_ids)</span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb115-12"><a href="#cb115-12" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(outputs.view(<span class="op">-</span><span class="dv">1</span>), targets)</span>
<span id="cb115-13"><a href="#cb115-13" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb115-14"><a href="#cb115-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-15"><a href="#cb115-15" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb115-16"><a href="#cb115-16" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb115-17"><a href="#cb115-17" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb115-18"><a href="#cb115-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_loss <span class="op">/</span> <span class="bu">len</span>(training_loader)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb116"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS):</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>    losses.append(train(epoch))</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;loss at epoch &quot;</span>, epoch, <span class="st">&quot;: &quot;</span>, losses[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/2500 [00:00&lt;?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to &#39;longest_first&#39; truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
100%|| 2500/2500 [15:08&lt;00:00,  2.75it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  0 :  0.6940998448610306
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [15:09&lt;00:00,  2.75it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  1 :  0.6933075151205063
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 2500/2500 [15:10&lt;00:00,  2.75it/s]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  2 :  0.6934334316968918
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
<div class="output display_data">
<pre><code>[&lt;matplotlib.lines.Line2D at 0x79f3ecc61d20&gt;]</code></pre>
</div>
<div class="output display_data">
<p><img src="6d31d0ca151561e91ee7703f8ff5967823f104f9.png" /></p>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb125"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> validation(epoch):</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>    fin_targets <span class="op">=</span> []</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a>    fin_outputs <span class="op">=</span> []</span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _, data <span class="kw">in</span> <span class="bu">enumerate</span>(testing_loader, <span class="dv">0</span>):</span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a>            ids <span class="op">=</span> data[<span class="st">&quot;ids&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb125-8"><a href="#cb125-8" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> data[<span class="st">&quot;mask&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb125-9"><a href="#cb125-9" aria-hidden="true" tabindex="-1"></a>            token_type_ids <span class="op">=</span> data[<span class="st">&quot;token_type_ids&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb125-10"><a href="#cb125-10" aria-hidden="true" tabindex="-1"></a>            targets <span class="op">=</span> data[<span class="st">&quot;targets&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb125-11"><a href="#cb125-11" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(ids, mask, token_type_ids)</span>
<span id="cb125-12"><a href="#cb125-12" aria-hidden="true" tabindex="-1"></a>            fin_targets.extend(targets.cpu().detach().numpy().tolist())</span>
<span id="cb125-13"><a href="#cb125-13" aria-hidden="true" tabindex="-1"></a>            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())</span>
<span id="cb125-14"><a href="#cb125-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fin_outputs, fin_targets</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb126"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>outputs, targets <span class="op">=</span> validation(<span class="dv">1</span>)</span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> np.array(outputs) <span class="op">&gt;=</span> <span class="fl">0.5</span></span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> metrics.accuracy_score(targets, outputs)</span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a>f1_score_micro <span class="op">=</span> metrics.f1_score(targets, outputs, average<span class="op">=</span><span class="st">&quot;micro&quot;</span>)</span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a>f1_score_macro <span class="op">=</span> metrics.f1_score(targets, outputs, average<span class="op">=</span><span class="st">&quot;macro&quot;</span>)</span>
<span id="cb126-6"><a href="#cb126-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Accuracy Score = </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb126-7"><a href="#cb126-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;F1 Score (Micro) = </span><span class="sc">{</span>f1_score_micro<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb126-8"><a href="#cb126-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;F1 Score (Macro) = </span><span class="sc">{</span>f1_score_macro<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Accuracy Score = 0.4938
F1 Score (Micro) = 0.4938
F1 Score (Macro) = 0.33056634087561926
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Our first obeservation is that this approach is very costly when it
comes to trainig time/memory requirements. Moreover, is seem that the
model is not able to learn anything from the data as the accuracy is
under 50% which is the accuracy of random guessing with bad luck...</p>
</div>
<div class="cell markdown">
<p>Let's also try with another set of weights, this time it is from the
AIRI institute.</p>
<p>The following code is pretty much the same as the one from the
previous section, we just load the weights from another model.</p>
<p>As the model is different, we need to change the dataset.</p>
</div>
<div class="cell code">
<div class="sourceCode" id="cb128"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install torch torchvision torchaudio transformers scikit<span class="op">-</span>learn matplotlib tqdm deepspeed</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb129"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel, BertConfig</span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModel</span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader, RandomSampler, SequentialSampler</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb129-8"><a href="#cb129-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb130"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sections of config</span></span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Defining some key variables that will be used later on in the training</span></span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a>MAX_LEN <span class="op">=</span> <span class="dv">90</span></span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true" tabindex="-1"></a>TRAIN_BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb130-6"><a href="#cb130-6" aria-hidden="true" tabindex="-1"></a>VALID_BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb130-7"><a href="#cb130-7" aria-hidden="true" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb130-8"><a href="#cb130-8" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">1e-02</span></span>
<span id="cb130-9"><a href="#cb130-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-10"><a href="#cb130-10" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(</span>
<span id="cb130-11"><a href="#cb130-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;AIRI-Institute/gena-lm-bert-base&quot;</span>, trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb130-12"><a href="#cb130-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb131"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomDataset(Dataset):</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dataframe, tokenizer, max_len):</span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> dataframe</span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.genome_sequence <span class="op">=</span> dataframe.genome_sequence</span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> dataframe.label.to_numpy()</span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_len <span class="op">=</span> max_len</span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-9"><a href="#cb131-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb131-10"><a href="#cb131-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.genome_sequence)</span>
<span id="cb131-11"><a href="#cb131-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-12"><a href="#cb131-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb131-13"><a href="#cb131-13" aria-hidden="true" tabindex="-1"></a>        genome_seq <span class="op">=</span> <span class="bu">str</span>(<span class="va">self</span>.genome_sequence.iloc[index])</span>
<span id="cb131-14"><a href="#cb131-14" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.tokenizer.encode_plus(</span>
<span id="cb131-15"><a href="#cb131-15" aria-hidden="true" tabindex="-1"></a>            genome_seq,</span>
<span id="cb131-16"><a href="#cb131-16" aria-hidden="true" tabindex="-1"></a>            <span class="va">None</span>,</span>
<span id="cb131-17"><a href="#cb131-17" aria-hidden="true" tabindex="-1"></a>            add_special_tokens<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb131-18"><a href="#cb131-18" aria-hidden="true" tabindex="-1"></a>            max_length<span class="op">=</span><span class="va">self</span>.max_len,</span>
<span id="cb131-19"><a href="#cb131-19" aria-hidden="true" tabindex="-1"></a>            pad_to_max_length<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb131-20"><a href="#cb131-20" aria-hidden="true" tabindex="-1"></a>            return_token_type_ids<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb131-21"><a href="#cb131-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb131-22"><a href="#cb131-22" aria-hidden="true" tabindex="-1"></a>        ids <span class="op">=</span> inputs[<span class="st">&quot;input_ids&quot;</span>]</span>
<span id="cb131-23"><a href="#cb131-23" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> inputs[<span class="st">&quot;attention_mask&quot;</span>]</span>
<span id="cb131-24"><a href="#cb131-24" aria-hidden="true" tabindex="-1"></a>        token_type_ids <span class="op">=</span> inputs[<span class="st">&quot;token_type_ids&quot;</span>]</span>
<span id="cb131-25"><a href="#cb131-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-26"><a href="#cb131-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb131-27"><a href="#cb131-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;ids&quot;</span>: torch.tensor(ids, dtype<span class="op">=</span>torch.<span class="bu">long</span>),</span>
<span id="cb131-28"><a href="#cb131-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;mask&quot;</span>: torch.tensor(mask, dtype<span class="op">=</span>torch.<span class="bu">long</span>),</span>
<span id="cb131-29"><a href="#cb131-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;token_type_ids&quot;</span>: torch.tensor(token_type_ids, dtype<span class="op">=</span>torch.<span class="bu">long</span>),</span>
<span id="cb131-30"><a href="#cb131-30" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;targets&quot;</span>: torch.tensor(<span class="va">self</span>.targets[index], dtype<span class="op">=</span>torch.<span class="bu">float</span>),</span>
<span id="cb131-31"><a href="#cb131-31" aria-hidden="true" tabindex="-1"></a>        }</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb132"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the dataset and dataloader for the neural network</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;FULL Dataset: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(df.shape))</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;TRAIN Dataset: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(df_train.shape))</span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;TEST Dataset: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(df_test.shape))</span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a>training_set <span class="op">=</span> CustomDataset(df_train, tokenizer, MAX_LEN)</span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>testing_set <span class="op">=</span> CustomDataset(df_test, tokenizer, MAX_LEN)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>FULL Dataset: (19799984, 2)
TRAIN Dataset: (80000, 2)
TEST Dataset: (20000, 2)
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb134"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>train_params <span class="op">=</span> {<span class="st">&quot;batch_size&quot;</span>: TRAIN_BATCH_SIZE, <span class="st">&quot;shuffle&quot;</span>: <span class="va">True</span>, <span class="st">&quot;num_workers&quot;</span>: <span class="dv">0</span>}</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a>test_params <span class="op">=</span> {<span class="st">&quot;batch_size&quot;</span>: VALID_BATCH_SIZE, <span class="st">&quot;shuffle&quot;</span>: <span class="va">True</span>, <span class="st">&quot;num_workers&quot;</span>: <span class="dv">0</span>}</span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a>training_loader <span class="op">=</span> DataLoader(training_set, <span class="op">**</span>train_params)</span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a>testing_loader <span class="op">=</span> DataLoader(testing_set, <span class="op">**</span>test_params)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb135"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb136"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model.</span></span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BERTClass(torch.nn.Module):</span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BERTClass, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l1 <span class="op">=</span> AutoModel.from_pretrained(</span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;AIRI-Institute/gena-lm-bert-base&quot;</span>, trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l2 <span class="op">=</span> torch.nn.Dropout(<span class="fl">0.3</span>)</span>
<span id="cb136-12"><a href="#cb136-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l3 <span class="op">=</span> torch.nn.Linear(<span class="dv">768</span>, <span class="dv">1</span>)</span>
<span id="cb136-13"><a href="#cb136-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-14"><a href="#cb136-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, ids, mask, token_type_ids):</span>
<span id="cb136-15"><a href="#cb136-15" aria-hidden="true" tabindex="-1"></a>        _, output_1 <span class="op">=</span> <span class="va">self</span>.l1(</span>
<span id="cb136-16"><a href="#cb136-16" aria-hidden="true" tabindex="-1"></a>            ids, attention_mask<span class="op">=</span>mask, token_type_ids<span class="op">=</span>token_type_ids, return_dict<span class="op">=</span><span class="va">False</span></span>
<span id="cb136-17"><a href="#cb136-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb136-18"><a href="#cb136-18" aria-hidden="true" tabindex="-1"></a>        output_2 <span class="op">=</span> <span class="va">self</span>.l2(output_1)</span>
<span id="cb136-19"><a href="#cb136-19" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.l3(output_2)</span>
<span id="cb136-20"><a href="#cb136-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-21"><a href="#cb136-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb136-22"><a href="#cb136-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-23"><a href="#cb136-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-24"><a href="#cb136-24" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BERTClass()</span>
<span id="cb136-25"><a href="#cb136-25" aria-hidden="true" tabindex="-1"></a>model.to(device)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>Some weights of BertModel were not initialized from the model checkpoint at AIRI-Institute/gena-lm-bert-base and are newly initialized: [&#39;bert.encoder.layer.3.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.3.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.9.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.6.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.0.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.1.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.10.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.7.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.11.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.7.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.11.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.3.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.10.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.1.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.1.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.8.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.11.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.9.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.2.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.0.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.3.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.6.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.2.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.10.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.2.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.6.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.4.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.4.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.8.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.11.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.7.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.4.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.5.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.8.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.6.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.0.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.2.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.8.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.4.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.10.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.5.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.5.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.0.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.9.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.7.attention.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.1.output.LayerNorm.weight&#39;, &#39;bert.encoder.layer.9.attention.output.LayerNorm.bias&#39;, &#39;bert.encoder.layer.5.output.LayerNorm.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
</div>
<div class="output display_data">
<pre><code>BERTClass(
  (l1): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(32000, 768, padding_idx=3)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (l2): Dropout(p=0.3, inplace=False)
  (l3): Linear(in_features=768, out_features=1, bias=True)
)</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb139"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss_fn(outputs, targets):</span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.nn.BCEWithLogitsLoss()(outputs, targets)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb140"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(params<span class="op">=</span>model.parameters(), lr<span class="op">=</span>LEARNING_RATE)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb141"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(epoch):</span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _, data <span class="kw">in</span> tqdm(<span class="bu">enumerate</span>(training_loader, <span class="dv">0</span>), total<span class="op">=</span><span class="bu">len</span>(training_loader)):</span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>        ids <span class="op">=</span> data[<span class="st">&quot;ids&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb141-6"><a href="#cb141-6" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> data[<span class="st">&quot;mask&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb141-7"><a href="#cb141-7" aria-hidden="true" tabindex="-1"></a>        token_type_ids <span class="op">=</span> data[<span class="st">&quot;token_type_ids&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb141-8"><a href="#cb141-8" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> data[<span class="st">&quot;targets&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb141-9"><a href="#cb141-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-10"><a href="#cb141-10" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(ids, mask, token_type_ids)</span>
<span id="cb141-11"><a href="#cb141-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-12"><a href="#cb141-12" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb141-13"><a href="#cb141-13" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(outputs.view(<span class="op">-</span><span class="dv">1</span>), targets)</span>
<span id="cb141-14"><a href="#cb141-14" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb141-15"><a href="#cb141-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-16"><a href="#cb141-16" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb141-17"><a href="#cb141-17" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb141-18"><a href="#cb141-18" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb141-19"><a href="#cb141-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_loss <span class="op">/</span> <span class="bu">len</span>(training_loader)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb142"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS):</span>
<span id="cb142-3"><a href="#cb142-3" aria-hidden="true" tabindex="-1"></a>    losses.append(train(epoch))</span>
<span id="cb142-4"><a href="#cb142-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;loss at epoch &quot;</span>, epoch, <span class="st">&quot;: &quot;</span>, losses[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb142-5"><a href="#cb142-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-6"><a href="#cb142-6" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/1250 [00:00&lt;?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to &#39;longest_first&#39; truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
100%|| 1250/1250 [10:36&lt;00:00,  1.96it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  0 :  0.775286752986908
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 1250/1250 [10:35&lt;00:00,  1.97it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  1 :  0.7864574189186097
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 1250/1250 [10:35&lt;00:00,  1.97it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  2 :  0.7764403200626373
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 1250/1250 [10:35&lt;00:00,  1.97it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  3 :  0.7774056327819824
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 1250/1250 [10:35&lt;00:00,  1.97it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  4 :  0.7812005206108094
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 1250/1250 [10:35&lt;00:00,  1.97it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  5 :  0.7675274605751038
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 1250/1250 [10:34&lt;00:00,  1.97it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  6 :  0.7817751158714294
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 1250/1250 [10:35&lt;00:00,  1.97it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  7 :  0.7843246402740478
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 1250/1250 [10:35&lt;00:00,  1.97it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  8 :  0.7757829966068268
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 1250/1250 [10:35&lt;00:00,  1.97it/s]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>loss at epoch  9 :  0.7828727591514587
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
<div class="output display_data">
<pre><code>[&lt;matplotlib.lines.Line2D at 0x783b78132e60&gt;]</code></pre>
</div>
<div class="output display_data">
<p><img src="5091e17744dde1d834808fe891e7d05646cb22b4.png" /></p>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb165"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> validation(epoch):</span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb165-3"><a href="#cb165-3" aria-hidden="true" tabindex="-1"></a>    fin_targets <span class="op">=</span> []</span>
<span id="cb165-4"><a href="#cb165-4" aria-hidden="true" tabindex="-1"></a>    fin_outputs <span class="op">=</span> []</span>
<span id="cb165-5"><a href="#cb165-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb165-6"><a href="#cb165-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _, data <span class="kw">in</span> <span class="bu">enumerate</span>(testing_loader, <span class="dv">0</span>):</span>
<span id="cb165-7"><a href="#cb165-7" aria-hidden="true" tabindex="-1"></a>            ids <span class="op">=</span> data[<span class="st">&quot;ids&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb165-8"><a href="#cb165-8" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> data[<span class="st">&quot;mask&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb165-9"><a href="#cb165-9" aria-hidden="true" tabindex="-1"></a>            token_type_ids <span class="op">=</span> data[<span class="st">&quot;token_type_ids&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb165-10"><a href="#cb165-10" aria-hidden="true" tabindex="-1"></a>            targets <span class="op">=</span> data[<span class="st">&quot;targets&quot;</span>].to(device, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb165-11"><a href="#cb165-11" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(ids, mask, token_type_ids)</span>
<span id="cb165-12"><a href="#cb165-12" aria-hidden="true" tabindex="-1"></a>            fin_targets.extend(targets.cpu().detach().numpy().tolist())</span>
<span id="cb165-13"><a href="#cb165-13" aria-hidden="true" tabindex="-1"></a>            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())</span>
<span id="cb165-14"><a href="#cb165-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fin_outputs, fin_targets</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb166"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a>outputs, targets <span class="op">=</span> validation(<span class="dv">1</span>)</span>
<span id="cb166-2"><a href="#cb166-2" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> np.array(outputs) <span class="op">&gt;=</span> <span class="fl">0.5</span></span>
<span id="cb166-3"><a href="#cb166-3" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> metrics.accuracy_score(targets, outputs)</span>
<span id="cb166-4"><a href="#cb166-4" aria-hidden="true" tabindex="-1"></a>f1_score_micro <span class="op">=</span> metrics.f1_score(targets, outputs, average<span class="op">=</span><span class="st">&quot;micro&quot;</span>)</span>
<span id="cb166-5"><a href="#cb166-5" aria-hidden="true" tabindex="-1"></a>f1_score_macro <span class="op">=</span> metrics.f1_score(targets, outputs, average<span class="op">=</span><span class="st">&quot;macro&quot;</span>)</span>
<span id="cb166-6"><a href="#cb166-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Accuracy Score = </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb166-7"><a href="#cb166-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;F1 Score (Micro) = </span><span class="sc">{</span>f1_score_micro<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb166-8"><a href="#cb166-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;F1 Score (Macro) = </span><span class="sc">{</span>f1_score_macro<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Accuracy Score = 0.4938
F1 Score (Micro) = 0.4938
F1 Score (Macro) = 0.33056634087561926
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Here we have the same bad results as the previous one.</p>
<p>It seems important to note that this model had much faster training
time which enabled for faster feedback. However, the results are still
not good.</p>
<p>Our approach on Bert is either wrong or the model is not able to
learn anything from the data provided.</p>
</div>
<div class="cell markdown">
<hr />
</div>
<section id="random-forest" class="cell markdown">
<h2>Random Forest</h2>
<p>Random Forest is a versatile and robust machine learning algorithm
capable of handling a mix of data types. By encoding the DNA sequences
appropriately, Random Forest can be utilized to identify the
distinguishing features between human and gorilla DNA.</p>
</section>
<div class="cell code" data-execution_count="3"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T16:20:21.348041Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T16:20:21.347384Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T16:20:31.336357Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T16:20:31.335090Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T16:20:21.348002Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb168"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install optuna scikit<span class="op">-</span>learn pandas numpy</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.3.0)
Requirement already satisfied: alembic&gt;=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.12.0)
Requirement already satisfied: cmaes&gt;=0.10.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (0.10.0)
Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.7.0)
Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.23.5)
Requirement already satisfied: packaging&gt;=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)
Requirement already satisfied: sqlalchemy&gt;=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.17)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.1)
Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0)
Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic&gt;=1.5.0-&gt;optuna) (1.2.4)
Requirement already satisfied: typing-extensions&gt;=4 in /opt/conda/lib/python3.10/site-packages (from alembic&gt;=1.5.0-&gt;optuna) (4.6.3)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging&gt;=20.0-&gt;optuna) (3.0.9)
Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy&gt;=1.3.0-&gt;optuna) (2.0.2)
Requirement already satisfied: MarkupSafe&gt;=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako-&gt;alembic&gt;=1.5.0-&gt;optuna) (2.1.3)
Note: you may need to restart the kernel to use updated packages.
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="23"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T16:24:11.796065Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T16:24:11.795708Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T16:24:11.801128Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T16:24:11.799882Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T16:24:11.796037Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb170"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb170-2"><a href="#cb170-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb170-3"><a href="#cb170-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb170-4"><a href="#cb170-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb170-5"><a href="#cb170-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb170-6"><a href="#cb170-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb170-7"><a href="#cb170-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb170-8"><a href="#cb170-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> HashingVectorizer</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="35"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T16:27:15.997135Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T16:27:15.996740Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T16:27:16.138750Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T16:27:16.137682Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T16:27:15.997105Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb171"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a>data_1000 <span class="op">=</span> data.sample(n<span class="op">=</span><span class="dv">1000000</span> <span class="op">//</span> <span class="dv">10</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="36"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T16:27:16.679560Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T16:27:16.678964Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T16:27:16.697403Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T16:27:16.696332Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T16:27:16.679528Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb172"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Getting the shape of the dataset</span></span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a>dataset_shape <span class="op">=</span> data_1000.shape</span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb172-4"><a href="#cb172-4" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;The dataset contains </span><span class="sc">{</span>dataset_shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> samples and </span><span class="sc">{</span>dataset_shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> columns.&quot;</span></span>
<span id="cb172-5"><a href="#cb172-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb172-6"><a href="#cb172-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-7"><a href="#cb172-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Checking the distribution of labels</span></span>
<span id="cb172-8"><a href="#cb172-8" aria-hidden="true" tabindex="-1"></a>label_distribution <span class="op">=</span> data_1000[<span class="st">&quot;species&quot;</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb172-9"><a href="#cb172-9" aria-hidden="true" tabindex="-1"></a>label_distribution</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The dataset contains 100000 samples and 4 columns.
</code></pre>
</div>
<div class="output execute_result" data-execution_count="36">
<pre><code>species
Homo_sapiens       0.50028
Gorilla_gorilla    0.49972
Name: proportion, dtype: float64</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="37"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T16:27:17.306094Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T16:27:17.305751Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T16:27:19.919954Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T16:27:19.918677Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T16:27:17.306066Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb175"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to convert a DNA sequence into overlapping k-mers</span></span>
<span id="cb175-2"><a href="#cb175-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kmerizer(seq, k<span class="op">=</span><span class="dv">6</span>):</span>
<span id="cb175-3"><a href="#cb175-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [seq[i : i <span class="op">+</span> k] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(seq) <span class="op">-</span> k <span class="op">+</span> <span class="dv">1</span>)]</span>
<span id="cb175-4"><a href="#cb175-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb175-5"><a href="#cb175-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb175-6"><a href="#cb175-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the function with a single DNA sequence</span></span>
<span id="cb175-7"><a href="#cb175-7" aria-hidden="true" tabindex="-1"></a><span class="co"># print(get_kmers(data_1000[&#39;genome_sequence&#39;][0]))</span></span>
<span id="cb175-8"><a href="#cb175-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb175-9"><a href="#cb175-9" aria-hidden="true" tabindex="-1"></a>hash_vectorizer <span class="op">=</span> HashingVectorizer(</span>
<span id="cb175-10"><a href="#cb175-10" aria-hidden="true" tabindex="-1"></a>    analyzer<span class="op">=</span>kmerizer, n_features<span class="op">=</span><span class="dv">2</span><span class="op">**</span><span class="dv">20</span></span>
<span id="cb175-11"><a href="#cb175-11" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Adjust n_features to manage memory usage</span></span>
<span id="cb175-12"><a href="#cb175-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb175-13"><a href="#cb175-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the vectorizer to the genome_sequence column of the DataFrame</span></span>
<span id="cb175-14"><a href="#cb175-14" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> hash_vectorizer.fit_transform(data_1000[<span class="st">&quot;genome_sequence&quot;</span>])</span>
<span id="cb175-15"><a href="#cb175-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb175-16"><a href="#cb175-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode species labels</span></span>
<span id="cb175-17"><a href="#cb175-17" aria-hidden="true" tabindex="-1"></a>data_1000[<span class="st">&quot;species_encoded&quot;</span>] <span class="op">=</span> data_1000[<span class="st">&quot;species&quot;</span>].<span class="bu">map</span>(</span>
<span id="cb175-18"><a href="#cb175-18" aria-hidden="true" tabindex="-1"></a>    {<span class="st">&quot;Homo_sapiens&quot;</span>: <span class="dv">0</span>, <span class="st">&quot;Gorilla_gorilla&quot;</span>: <span class="dv">1</span>}</span>
<span id="cb175-19"><a href="#cb175-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb175-20"><a href="#cb175-20" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data_1000[<span class="st">&quot;species_encoded&quot;</span>]</span>
<span id="cb175-21"><a href="#cb175-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb175-22"><a href="#cb175-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Split dataset into training and testing sets</span></span>
<span id="cb175-23"><a href="#cb175-23" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb175-24"><a href="#cb175-24" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb175-25"><a href="#cb175-25" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="43"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T16:32:36.544870Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T16:32:36.544197Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T16:35:53.957939Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T16:35:53.956785Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T16:32:36.544841Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb176"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(trial):</span>
<span id="cb176-2"><a href="#cb176-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hyperparameters to be optimized</span></span>
<span id="cb176-3"><a href="#cb176-3" aria-hidden="true" tabindex="-1"></a>    n_estimators <span class="op">=</span> trial.suggest_int(<span class="st">&quot;n_estimators&quot;</span>, <span class="dv">10</span>, <span class="dv">200</span>)  <span class="co"># Expanded range</span></span>
<span id="cb176-4"><a href="#cb176-4" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> trial.suggest_categorical(<span class="st">&quot;criterion&quot;</span>, [<span class="st">&quot;gini&quot;</span>, <span class="st">&quot;entropy&quot;</span>, <span class="st">&quot;log_loss&quot;</span>])</span>
<span id="cb176-5"><a href="#cb176-5" aria-hidden="true" tabindex="-1"></a>    max_depth <span class="op">=</span> trial.suggest_int(<span class="st">&quot;max_depth&quot;</span>, <span class="dv">2</span>, <span class="dv">32</span>, log<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb176-6"><a href="#cb176-6" aria-hidden="true" tabindex="-1"></a>    min_samples_split <span class="op">=</span> trial.suggest_float(<span class="st">&quot;min_samples_split&quot;</span>, <span class="fl">0.1</span>, <span class="dv">1</span>)</span>
<span id="cb176-7"><a href="#cb176-7" aria-hidden="true" tabindex="-1"></a>    min_samples_leaf <span class="op">=</span> trial.suggest_float(<span class="st">&quot;min_samples_leaf&quot;</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>)</span>
<span id="cb176-8"><a href="#cb176-8" aria-hidden="true" tabindex="-1"></a>    min_weight_fraction_leaf <span class="op">=</span> trial.suggest_float(<span class="st">&quot;min_weight_fraction_leaf&quot;</span>, <span class="fl">0.0</span>, <span class="fl">0.5</span>)</span>
<span id="cb176-9"><a href="#cb176-9" aria-hidden="true" tabindex="-1"></a>    max_leaf_nodes <span class="op">=</span> trial.suggest_int(<span class="st">&quot;max_leaf_nodes&quot;</span>, <span class="dv">2</span>, <span class="dv">128</span>, log<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb176-10"><a href="#cb176-10" aria-hidden="true" tabindex="-1"></a>    min_impurity_decrease <span class="op">=</span> trial.suggest_float(<span class="st">&quot;min_impurity_decrease&quot;</span>, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb176-11"><a href="#cb176-11" aria-hidden="true" tabindex="-1"></a>    bootstrap <span class="op">=</span> trial.suggest_categorical(<span class="st">&quot;bootstrap&quot;</span>, [<span class="va">True</span>, <span class="va">False</span>])</span>
<span id="cb176-12"><a href="#cb176-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-13"><a href="#cb176-13" aria-hidden="true" tabindex="-1"></a>    classifier <span class="op">=</span> RandomForestClassifier(</span>
<span id="cb176-14"><a href="#cb176-14" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span>n_estimators,</span>
<span id="cb176-15"><a href="#cb176-15" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb176-16"><a href="#cb176-16" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span>max_depth,</span>
<span id="cb176-17"><a href="#cb176-17" aria-hidden="true" tabindex="-1"></a>        min_samples_split<span class="op">=</span>min_samples_split,</span>
<span id="cb176-18"><a href="#cb176-18" aria-hidden="true" tabindex="-1"></a>        min_samples_leaf<span class="op">=</span>min_samples_leaf,</span>
<span id="cb176-19"><a href="#cb176-19" aria-hidden="true" tabindex="-1"></a>        min_weight_fraction_leaf<span class="op">=</span>min_weight_fraction_leaf,</span>
<span id="cb176-20"><a href="#cb176-20" aria-hidden="true" tabindex="-1"></a>        max_leaf_nodes<span class="op">=</span>max_leaf_nodes,</span>
<span id="cb176-21"><a href="#cb176-21" aria-hidden="true" tabindex="-1"></a>        min_impurity_decrease<span class="op">=</span>min_impurity_decrease,</span>
<span id="cb176-22"><a href="#cb176-22" aria-hidden="true" tabindex="-1"></a>        bootstrap<span class="op">=</span>bootstrap,</span>
<span id="cb176-23"><a href="#cb176-23" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb176-24"><a href="#cb176-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb176-25"><a href="#cb176-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-26"><a href="#cb176-26" aria-hidden="true" tabindex="-1"></a>    classifier.fit(X_train, y_train)</span>
<span id="cb176-27"><a href="#cb176-27" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> classifier.predict(X_test)</span>
<span id="cb176-28"><a href="#cb176-28" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> accuracy_score(y_test, predictions)</span>
<span id="cb176-29"><a href="#cb176-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> accuracy</span>
<span id="cb176-30"><a href="#cb176-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-31"><a href="#cb176-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-32"><a href="#cb176-32" aria-hidden="true" tabindex="-1"></a>optuna.logging.set_verbosity(optuna.logging.INFO)</span>
<span id="cb176-33"><a href="#cb176-33" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study(direction<span class="op">=</span><span class="st">&quot;maximize&quot;</span>)</span>
<span id="cb176-34"><a href="#cb176-34" aria-hidden="true" tabindex="-1"></a>study.optimize(objective, n_trials<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb176-35"><a href="#cb176-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-36"><a href="#cb176-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Results of the optimization</span></span>
<span id="cb176-37"><a href="#cb176-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Number of finished trials: </span><span class="sc">{</span><span class="bu">len</span>(study.trials)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb176-38"><a href="#cb176-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Best trial: </span><span class="sc">{</span>study<span class="sc">.</span>best_trial<span class="sc">.</span>params<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>[I 2023-10-14 16:32:36,549] A new study created in memory with name: no-name-79f3cd72-899e-40a2-b59b-bcd6eb355275
[I 2023-10-14 16:32:39,620] Trial 0 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 158, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 17, &#39;min_samples_split&#39;: 0.3880243171996576, &#39;min_samples_leaf&#39;: 0.4247838122008928, &#39;min_weight_fraction_leaf&#39;: 0.170287707952885, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 7, &#39;min_impurity_decrease&#39;: 0.6467542020296969, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:32:40,715] Trial 1 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 52, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 0.44074976665665155, &#39;min_samples_leaf&#39;: 0.34299833723703405, &#39;min_weight_fraction_leaf&#39;: 0.10290971441725894, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 43, &#39;min_impurity_decrease&#39;: 0.6302062786681768, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:32:41,472] Trial 2 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 33, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 0.32432136464410743, &#39;min_samples_leaf&#39;: 0.14442764463945545, &#39;min_weight_fraction_leaf&#39;: 0.32342390619851286, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 2, &#39;min_impurity_decrease&#39;: 0.4434399406963455, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:32:44,567] Trial 3 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 163, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 0.11198761313685379, &#39;min_samples_leaf&#39;: 0.41279906821281565, &#39;min_weight_fraction_leaf&#39;: 0.24361208195277767, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 6, &#39;min_impurity_decrease&#39;: 0.12083561398325782, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:32:45,402] Trial 4 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 35, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 0.2849055239040029, &#39;min_samples_leaf&#39;: 0.4210652164704197, &#39;min_weight_fraction_leaf&#39;: 0.09901015366556265, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 61, &#39;min_impurity_decrease&#39;: 0.8171800125686763, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:32:46,776] Trial 5 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 67, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 0.3668027502782485, &#39;min_samples_leaf&#39;: 0.20838706376290098, &#39;min_weight_fraction_leaf&#39;: 0.34456922877704144, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 5, &#39;min_impurity_decrease&#39;: 0.33241204783473677, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:32:48,127] Trial 6 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 62, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 13, &#39;min_samples_split&#39;: 0.42717651635221665, &#39;min_samples_leaf&#39;: 0.24182270485086838, &#39;min_weight_fraction_leaf&#39;: 0.2174859684450981, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 10, &#39;min_impurity_decrease&#39;: 0.03988964896269953, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:32:51,576] Trial 7 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 153, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 28, &#39;min_samples_split&#39;: 0.5350003309396081, &#39;min_samples_leaf&#39;: 0.22695107267479894, &#39;min_weight_fraction_leaf&#39;: 0.23551248957093918, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 87, &#39;min_impurity_decrease&#39;: 0.19966672270803043, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:32:53,974] Trial 8 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 120, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 0.23734093021932653, &#39;min_samples_leaf&#39;: 0.2610330166593675, &#39;min_weight_fraction_leaf&#39;: 0.020146752371912213, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 40, &#39;min_impurity_decrease&#39;: 0.39152251552950745, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:32:54,934] Trial 9 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 44, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 0.5673002359018897, &#39;min_samples_leaf&#39;: 0.25377603693815326, &#39;min_weight_fraction_leaf&#39;: 0.15244490998086307, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 3, &#39;min_impurity_decrease&#39;: 0.6241885877006351, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:32:59,356] Trial 10 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 190, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 32, &#39;min_samples_split&#39;: 0.7875475395472189, &#39;min_samples_leaf&#39;: 0.35610709880975017, &#39;min_weight_fraction_leaf&#39;: 0.4828800447502953, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 17, &#39;min_impurity_decrease&#39;: 0.7733068206130589, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:01,265] Trial 11 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 96, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 14, &#39;min_samples_split&#39;: 0.5563491062551652, &#39;min_samples_leaf&#39;: 0.4984630116695839, &#39;min_weight_fraction_leaf&#39;: 0.09393305626569978, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 27, &#39;min_impurity_decrease&#39;: 0.9916008681144797, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:03,470] Trial 12 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 111, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 15, &#39;min_samples_split&#39;: 0.9867659546016241, &#39;min_samples_leaf&#39;: 0.33195012140526975, &#39;min_weight_fraction_leaf&#39;: 0.03361472952885025, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 15, &#39;min_impurity_decrease&#39;: 0.5643864993498957, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:03,904] Trial 13 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 13, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 9, &#39;min_samples_split&#39;: 0.4751930635804304, &#39;min_samples_leaf&#39;: 0.37260452819893547, &#39;min_weight_fraction_leaf&#39;: 0.16568200597890467, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 111, &#39;min_impurity_decrease&#39;: 0.6169711433149067, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:06,577] Trial 14 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 136, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 0.692058149855698, &#39;min_samples_leaf&#39;: 0.3215661041817209, &#39;min_weight_fraction_leaf&#39;: 0.0970831177044085, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 35, &#39;min_impurity_decrease&#39;: 0.5133925400185395, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:10,231] Trial 15 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 194, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 21, &#39;min_samples_split&#39;: 0.4260927302778763, &#39;min_samples_leaf&#39;: 0.45911213871356304, &#39;min_weight_fraction_leaf&#39;: 0.12439289472109698, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 10, &#39;min_impurity_decrease&#39;: 0.7129383399032205, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:11,939] Trial 16 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 85, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 10, &#39;min_samples_split&#39;: 0.19929383519736557, &#39;min_samples_leaf&#39;: 0.39503831527859745, &#39;min_weight_fraction_leaf&#39;: 0.005398772906051119, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 24, &#39;min_impurity_decrease&#39;: 0.7025400867540506, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:15,387] Trial 17 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 168, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 19, &#39;min_samples_split&#39;: 0.351346582868861, &#39;min_samples_leaf&#39;: 0.3007866252229398, &#39;min_weight_fraction_leaf&#39;: 0.18916580873655714, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 54, &#39;min_impurity_decrease&#39;: 0.48785581076397505, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:17,897] Trial 18 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 128, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 10, &#39;min_samples_split&#39;: 0.15807783940746373, &#39;min_samples_leaf&#39;: 0.4469831285354705, &#39;min_weight_fraction_leaf&#39;: 0.05953740974162959, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 21, &#39;min_impurity_decrease&#39;: 0.3000455988480033, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:19,530] Trial 19 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 82, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 22, &#39;min_samples_split&#39;: 0.2660651648077958, &#39;min_samples_leaf&#39;: 0.36317257303185535, &#39;min_weight_fraction_leaf&#39;: 0.14740483415144334, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 11, &#39;min_impurity_decrease&#39;: 0.8684941443804228, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:22,466] Trial 20 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 143, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 0.20649934912389678, &#39;min_samples_leaf&#39;: 0.2919354272179343, &#39;min_weight_fraction_leaf&#39;: 0.18310865826394584, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 6, &#39;min_impurity_decrease&#39;: 0.5980885075429674, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:22,902] Trial 21 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 13, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 0.3283144741093965, &#39;min_samples_leaf&#39;: 0.11483011405203697, &#39;min_weight_fraction_leaf&#39;: 0.2809280519780693, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 3, &#39;min_impurity_decrease&#39;: 0.46784539329560004, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:23,741] Trial 22 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 35, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 0.36813693543687787, &#39;min_samples_leaf&#39;: 0.16629545745570362, &#39;min_weight_fraction_leaf&#39;: 0.28027141428792135, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 2, &#39;min_impurity_decrease&#39;: 0.4395682751731772, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:24,883] Trial 23 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 52, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 0.2891357838615928, &#39;min_samples_leaf&#39;: 0.17243657370500193, &#39;min_weight_fraction_leaf&#39;: 0.33033960457405903, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 2, &#39;min_impurity_decrease&#39;: 0.5406386973622795, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:26,399] Trial 24 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 72, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 0.45013536450206537, &#39;min_samples_leaf&#39;: 0.29136830674958475, &#39;min_weight_fraction_leaf&#39;: 0.05677302830128282, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 8, &#39;min_impurity_decrease&#39;: 0.6700051619595114, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:27,126] Trial 25 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 25, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 4, &#39;min_samples_split&#39;: 0.37575963778275095, &#39;min_samples_leaf&#39;: 0.34484361345755343, &#39;min_weight_fraction_leaf&#39;: 0.19939713187625674, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 4, &#39;min_impurity_decrease&#39;: 0.40213656944405457, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:29,534] Trial 26 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 103, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 8, &#39;min_samples_split&#39;: 0.2969312890899545, &#39;min_samples_leaf&#39;: 0.3870952394224016, &#39;min_weight_fraction_leaf&#39;: 0.12809338767943826, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 15, &#39;min_impurity_decrease&#39;: 0.5427976727518339, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:30,668] Trial 27 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 53, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 6, &#39;min_samples_split&#39;: 0.5075626911486021, &#39;min_samples_leaf&#39;: 0.33640911960077563, &#39;min_weight_fraction_leaf&#39;: 0.1722189640168989, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 3, &#39;min_impurity_decrease&#39;: 0.6458768940600765, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:33,960] Trial 28 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 172, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 12, &#39;min_samples_split&#39;: 0.3925088802214414, &#39;min_samples_leaf&#39;: 0.11284037356601884, &#39;min_weight_fraction_leaf&#39;: 0.07008861852315762, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 7, &#39;min_impurity_decrease&#39;: 0.7321799801193077, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:37,501] Trial 29 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 181, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 4, &#39;min_samples_split&#39;: 0.14793917868555362, &#39;min_samples_leaf&#39;: 0.405465799606231, &#39;min_weight_fraction_leaf&#39;: 0.26820732851581214, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 5, &#39;min_impurity_decrease&#39;: 0.5584143257679843, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:40,485] Trial 30 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 155, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 11, &#39;min_samples_split&#39;: 0.228966381125303, &#39;min_samples_leaf&#39;: 0.41997864583681627, &#39;min_weight_fraction_leaf&#39;: 0.24118732501544282, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 8, &#39;min_impurity_decrease&#39;: 0.4655547157257416, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:41,251] Trial 31 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 31, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 8, &#39;min_samples_split&#39;: 0.16806801877665767, &#39;min_samples_leaf&#39;: 0.3800470586976944, &#39;min_weight_fraction_leaf&#39;: 0.13131025772891683, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 4, &#39;min_impurity_decrease&#39;: 0.181806763877525, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:44,316] Trial 32 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 160, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 6, &#39;min_samples_split&#39;: 0.30713680811307237, &#39;min_samples_leaf&#39;: 0.43078225655453656, &#39;min_weight_fraction_leaf&#39;: 0.2165039754549523, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 12, &#39;min_impurity_decrease&#39;: 0.0024824184060687066, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:47,125] Trial 33 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 144, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 4, &#39;min_samples_split&#39;: 0.1202904670415828, &#39;min_samples_leaf&#39;: 0.3979013755721424, &#39;min_weight_fraction_leaf&#39;: 0.314045615587883, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 6, &#39;min_impurity_decrease&#39;: 0.32236480735064166, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:48,443] Trial 34 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 63, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 0.11409335724415484, &#39;min_samples_leaf&#39;: 0.36679022116140825, &#39;min_weight_fraction_leaf&#39;: 0.21926927447051217, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 8, &#39;min_impurity_decrease&#39;: 0.09546201967665204, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:50,833] Trial 35 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 120, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 0.10619186536655278, &#39;min_samples_leaf&#39;: 0.4162016224570665, &#39;min_weight_fraction_leaf&#39;: 0.36531982298475435, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 12, &#39;min_impurity_decrease&#39;: 0.2599717843366653, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:33:51,902] Trial 36 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 46, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 9, &#39;min_samples_split&#39;: 0.24783748594133262, &#39;min_samples_leaf&#39;: 0.2750146381926115, &#39;min_weight_fraction_leaf&#39;: 0.24057960659268368, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 78, &#39;min_impurity_decrease&#39;: 0.09802281725360082, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:56,089] Trial 37 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 181, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 15, &#39;min_samples_split&#39;: 0.41568617465559815, &#39;min_samples_leaf&#39;: 0.311390676302354, &#39;min_weight_fraction_leaf&#39;: 0.11030524975335536, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 2, &#39;min_impurity_decrease&#39;: 0.4024733226193755, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:33:56,739] Trial 38 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 23, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 0.3260817574217848, &#39;min_samples_leaf&#39;: 0.24524211479438585, &#39;min_weight_fraction_leaf&#39;: 0.14909064107106584, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 19, &#39;min_impurity_decrease&#39;: 0.2204635327417933, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:00,776] Trial 39 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 199, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 13, &#39;min_samples_split&#39;: 0.25984424057146105, &#39;min_samples_leaf&#39;: 0.34365986758491784, &#39;min_weight_fraction_leaf&#39;: 0.1992328012367001, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 13, &#39;min_impurity_decrease&#39;: 0.36101143001641617, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:02,409] Trial 40 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 75, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 6, &#39;min_samples_split&#39;: 0.4541044377703619, &#39;min_samples_leaf&#39;: 0.2204709796602206, &#39;min_weight_fraction_leaf&#39;: 0.3616579606237851, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 35, &#39;min_impurity_decrease&#39;: 0.5876157345783006, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:03,360] Trial 41 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 42, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 32, &#39;min_samples_split&#39;: 0.3468207680115355, &#39;min_samples_leaf&#39;: 0.46204832673952606, &#39;min_weight_fraction_leaf&#39;: 0.08780467298433224, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 59, &#39;min_impurity_decrease&#39;: 0.7823671832746664, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:04,572] Trial 42 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 57, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 17, &#39;min_samples_split&#39;: 0.3861536505634058, &#39;min_samples_leaf&#39;: 0.43471460877794843, &#39;min_weight_fraction_leaf&#39;: 0.11376733088124558, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 28, &#39;min_impurity_decrease&#39;: 0.6459284529638146, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:05,470] Trial 43 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 37, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 25, &#39;min_samples_split&#39;: 0.2134690150103345, &#39;min_samples_leaf&#39;: 0.38534865850525774, &#39;min_weight_fraction_leaf&#39;: 0.025756148131528947, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 45, &#39;min_impurity_decrease&#39;: 0.8098033823138303, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:06,103] Trial 44 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 24, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 12, &#39;min_samples_split&#39;: 0.4950813004278356, &#39;min_samples_leaf&#39;: 0.3598912111976474, &#39;min_weight_fraction_leaf&#39;: 0.16979256677035443, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 95, &#39;min_impurity_decrease&#39;: 0.5163374352675629, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:08,036] Trial 45 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 93, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 8, &#39;min_samples_split&#39;: 0.4328193920180629, &#39;min_samples_leaf&#39;: 0.4141936291407341, &#39;min_weight_fraction_leaf&#39;: 0.0858123116882789, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 127, &#39;min_impurity_decrease&#39;: 0.6898025068831413, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:11,166] Trial 46 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 165, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 16, &#39;min_samples_split&#39;: 0.2749000283567001, &#39;min_samples_leaf&#39;: 0.4796842957972497, &#39;min_weight_fraction_leaf&#39;: 0.1483199827764075, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 10, &#39;min_impurity_decrease&#39;: 0.5799457659415855, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:11,654] Trial 47 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 16, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 13, &#39;min_samples_split&#39;: 0.32538825919661735, &#39;min_samples_leaf&#39;: 0.3226083123414598, &#39;min_weight_fraction_leaf&#39;: 0.03566514409116929, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 66, &#39;min_impurity_decrease&#39;: 0.6257701492458294, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:13,896] Trial 48 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 114, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 10, &#39;min_samples_split&#39;: 0.1813659276559747, &#39;min_samples_leaf&#39;: 0.4013185137117174, &#39;min_weight_fraction_leaf&#39;: 0.09661925898082248, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 44, &#39;min_impurity_decrease&#39;: 0.7447003083049015, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:16,582] Trial 49 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 135, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 19, &#39;min_samples_split&#39;: 0.5414217301642893, &#39;min_samples_leaf&#39;: 0.43661699475793947, &#39;min_weight_fraction_leaf&#39;: 0.1594287508435631, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 16, &#39;min_impurity_decrease&#39;: 0.8492769852310299, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:17,598] Trial 50 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 46, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 0.21477642438010122, &#39;min_samples_leaf&#39;: 0.3717691249412727, &#39;min_weight_fraction_leaf&#39;: 0.11377207193179346, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 20, &#39;min_impurity_decrease&#39;: 0.6851225050728041, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:18,988] Trial 51 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 66, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 0.3934310991851116, &#39;min_samples_leaf&#39;: 0.4896315572955359, &#39;min_weight_fraction_leaf&#39;: 0.256805257525653, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 4, &#39;min_impurity_decrease&#39;: 0.43960194982185247, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:20,797] Trial 52 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 87, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 25, &#39;min_samples_split&#39;: 0.3432147940587272, &#39;min_samples_leaf&#39;: 0.4510744122810881, &#39;min_weight_fraction_leaf&#39;: 0.3913846264243035, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 5, &#39;min_impurity_decrease&#39;: 0.5129234132764, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:21,587] Trial 53 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 32, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 29, &#39;min_samples_split&#39;: 0.27688133128012826, &#39;min_samples_leaf&#39;: 0.468472330645973, &#39;min_weight_fraction_leaf&#39;: 0.2999833876643324, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 3, &#39;min_impurity_decrease&#39;: 0.36592560737128754, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:23,128] Trial 54 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 74, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 0.4718242996737997, &#39;min_samples_leaf&#39;: 0.4463370737466665, &#39;min_weight_fraction_leaf&#39;: 0.21345593952638595, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 10, &#39;min_impurity_decrease&#39;: 0.9172045692836315, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:24,363] Trial 55 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 57, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 0.30574210759139736, &#39;min_samples_leaf&#39;: 0.4210023454990222, &#39;min_weight_fraction_leaf&#39;: 0.1804145755557112, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 9, &#39;min_impurity_decrease&#39;: 0.6107012428710837, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:25,356] Trial 56 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 40, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 4, &#39;min_samples_split&#39;: 0.2444261762444003, &#39;min_samples_leaf&#39;: 0.3938492939743984, &#39;min_weight_fraction_leaf&#39;: 0.13895774095842917, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 6, &#39;min_impurity_decrease&#39;: 0.7110573157330944, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:28,345] Trial 57 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 145, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 0.36118736348581526, &#39;min_samples_leaf&#39;: 0.21706160189639312, &#39;min_weight_fraction_leaf&#39;: 0.25706550491096647, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 29, &#39;min_impurity_decrease&#39;: 0.6558448955590376, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:32,057] Trial 58 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 185, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 14, &#39;min_samples_split&#39;: 0.5911187513336746, &#39;min_samples_leaf&#39;: 0.3769984619658239, &#39;min_weight_fraction_leaf&#39;: 0.06929707106230568, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 7, &#39;min_impurity_decrease&#39;: 0.48600297152348937, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:33,111] Trial 59 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 47, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 6, &#39;min_samples_split&#39;: 0.4128334067824486, &#39;min_samples_leaf&#39;: 0.3479948496801546, &#39;min_weight_fraction_leaf&#39;: 0.23307605524856367, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 13, &#39;min_impurity_decrease&#39;: 0.5498184648981524, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:36,400] Trial 60 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 171, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 17, &#39;min_samples_split&#39;: 0.36591755313208957, &#39;min_samples_leaf&#39;: 0.35672794948556874, &#39;min_weight_fraction_leaf&#39;: 0.15985368199493735, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 5, &#39;min_impurity_decrease&#39;: 0.4379001077256073, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:38,684] Trial 61 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 102, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 9, &#39;min_samples_split&#39;: 0.42454681160195873, &#39;min_samples_leaf&#39;: 0.20141927090104816, &#39;min_weight_fraction_leaf&#39;: 0.1294588941194785, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 11, &#39;min_impurity_decrease&#39;: 0.7419013128401606, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:40,164] Trial 62 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 67, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 19, &#39;min_samples_split&#39;: 0.2949412334085857, &#39;min_samples_leaf&#39;: 0.2553503041712669, &#39;min_weight_fraction_leaf&#39;: 0.18853476362032257, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 17, &#39;min_impurity_decrease&#39;: 0.6243069168757667, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:41,357] Trial 63 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 52, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 11, &#39;min_samples_split&#39;: 0.4510189739902998, &#39;min_samples_leaf&#39;: 0.14231242043773668, &#39;min_weight_fraction_leaf&#39;: 0.28466519121352746, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 23, &#39;min_impurity_decrease&#39;: 0.13532398968616238, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:43,106] Trial 64 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 80, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 0.40150736232841, &#39;min_samples_leaf&#39;: 0.271914603479496, &#39;min_weight_fraction_leaf&#39;: 0.19451877246759303, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 9, &#39;min_impurity_decrease&#39;: 0.004962850016291084, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:43,849] Trial 65 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 29, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 4, &#39;min_samples_split&#39;: 0.14289167011611356, &#39;min_samples_leaf&#39;: 0.1890808639893633, &#39;min_weight_fraction_leaf&#39;: 0.21886588647879962, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 14, &#39;min_impurity_decrease&#39;: 0.27044819191335623, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:44,438] Trial 66 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 19, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 8, &#39;min_samples_split&#39;: 0.3267534640529613, &#39;min_samples_leaf&#39;: 0.14738714355702326, &#39;min_weight_fraction_leaf&#39;: 0.17705487838895026, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 7, &#39;min_impurity_decrease&#39;: 0.33284554142558365, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:45,861] Trial 67 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 60, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 0.36553235830507325, &#39;min_samples_leaf&#39;: 0.23140819312356095, &#39;min_weight_fraction_leaf&#39;: 0.26745094429301114, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 76, &#39;min_impurity_decrease&#39;: 0.5834186509684409, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:48,742] Trial 68 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 149, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 0.1908907375538756, &#39;min_samples_leaf&#39;: 0.23793570714716603, &#39;min_weight_fraction_leaf&#39;: 0.0011659093715099822, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 56, &#39;min_impurity_decrease&#39;: 0.5274285895528437, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:49,150] Trial 69 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 11, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 14, &#39;min_samples_split&#39;: 0.24906020256291495, &#39;min_samples_leaf&#39;: 0.4064045180555341, &#39;min_weight_fraction_leaf&#39;: 0.23066594488561737, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 2, &#39;min_impurity_decrease&#39;: 0.042303980634749834, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:34:52,421] Trial 70 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 159, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 10, &#39;min_samples_split&#39;: 0.23198369735415664, &#39;min_samples_leaf&#39;: 0.10074983195184664, &#39;min_weight_fraction_leaf&#39;: 0.20428993496364684, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 4, &#39;min_impurity_decrease&#39;: 0.19024357239062972, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:55,278] Trial 71 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 135, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 23, &#39;min_samples_split&#39;: 0.43436081997894166, &#39;min_samples_leaf&#39;: 0.25732508053324843, &#39;min_weight_fraction_leaf&#39;: 0.25010572949966287, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 88, &#39;min_impurity_decrease&#39;: 0.13723421289560228, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:34:58,933] Trial 72 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 178, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 17, &#39;min_samples_split&#39;: 0.3849040307697923, &#39;min_samples_leaf&#39;: 0.291467348025404, &#39;min_weight_fraction_leaf&#39;: 0.22732957469539658, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 102, &#39;min_impurity_decrease&#39;: 0.3043278122402351, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:02,391] Trial 73 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 157, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 29, &#39;min_samples_split&#39;: 0.4740150873912463, &#39;min_samples_leaf&#39;: 0.20708918695575235, &#39;min_weight_fraction_leaf&#39;: 0.24380783575244644, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 51, &#39;min_impurity_decrease&#39;: 0.23895045352122002, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:05,159] Trial 74 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 126, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 15, &#39;min_samples_split&#39;: 0.5145043762819451, &#39;min_samples_leaf&#39;: 0.2463736108887002, &#39;min_weight_fraction_leaf&#39;: 0.20888178499609145, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 65, &#39;min_impurity_decrease&#39;: 0.49015792235450273, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:08,414] Trial 75 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 152, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 20, &#39;min_samples_split&#39;: 0.3432647761764043, &#39;min_samples_leaf&#39;: 0.30647966967622475, &#39;min_weight_fraction_leaf&#39;: 0.19942422475186097, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 35, &#39;min_impurity_decrease&#39;: 0.6718895948029356, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:11,670] Trial 76 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 166, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 22, &#39;min_samples_split&#39;: 0.2785690746487976, &#39;min_samples_leaf&#39;: 0.27751656666177704, &#39;min_weight_fraction_leaf&#39;: 0.16655223850630443, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 5, &#39;min_impurity_decrease&#39;: 0.07540799975581795, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:35:12,537] Trial 77 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 36, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 20, &#39;min_samples_split&#39;: 0.3086440841621835, &#39;min_samples_leaf&#39;: 0.3315722923047183, &#39;min_weight_fraction_leaf&#39;: 0.15023011124877406, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 86, &#39;min_impurity_decrease&#39;: 0.5478304835600455, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:13,766] Trial 78 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 52, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 6, &#39;min_samples_split&#39;: 0.38935100498214475, &#39;min_samples_leaf&#39;: 0.22839648842324473, &#39;min_weight_fraction_leaf&#39;: 0.18903569890900085, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 6, &#39;min_impurity_decrease&#39;: 0.13950877755145702, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:35:14,766] Trial 79 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 42, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 12, &#39;min_samples_split&#39;: 0.4137680112827161, &#39;min_samples_leaf&#39;: 0.38688623801913363, &#39;min_weight_fraction_leaf&#39;: 0.11141126891664366, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 113, &#39;min_impurity_decrease&#39;: 0.1736533639691476, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:16,316] Trial 80 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 69, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 18, &#39;min_samples_split&#39;: 0.17194178279751648, &#39;min_samples_leaf&#39;: 0.26517262217461607, &#39;min_weight_fraction_leaf&#39;: 0.32969536674884015, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 15, &#39;min_impurity_decrease&#39;: 0.7216136917132483, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:18,844] Trial 81 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 127, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 9, &#39;min_samples_split&#39;: 0.20503386749308503, &#39;min_samples_leaf&#39;: 0.24165601949234652, &#39;min_weight_fraction_leaf&#39;: 0.039686249525849815, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 3, &#39;min_impurity_decrease&#39;: 0.40140508145469755, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:21,074] Trial 82 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 111, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 11, &#39;min_samples_split&#39;: 0.13036140822374803, &#39;min_samples_leaf&#39;: 0.42994630287890534, &#39;min_weight_fraction_leaf&#39;: 0.015431037583536311, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 48, &#39;min_impurity_decrease&#39;: 0.2884408405419579, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:23,773] Trial 83 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 140, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 0.26311394370392915, &#39;min_samples_leaf&#39;: 0.24862256288848447, &#39;min_weight_fraction_leaf&#39;: 0.0490993279852268, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 39, &#39;min_impurity_decrease&#39;: 0.4583678913971041, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:26,999] Trial 84 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 162, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 16, &#39;min_samples_split&#39;: 0.3485855150619266, &#39;min_samples_leaf&#39;: 0.22552698724209808, &#39;min_weight_fraction_leaf&#39;: 0.08599635336258696, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 63, &#39;min_impurity_decrease&#39;: 0.7743916634963459, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:35:30,360] Trial 85 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 174, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 0.15595969880964514, &#39;min_samples_leaf&#39;: 0.28447627479924714, &#39;min_weight_fraction_leaf&#39;: 0.07533251411426514, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 73, &#39;min_impurity_decrease&#39;: 0.573493634365984, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:32,276] Trial 86 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 89, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 6, &#39;min_samples_split&#39;: 0.30697548293486027, &#39;min_samples_leaf&#39;: 0.2626116672102698, &#39;min_weight_fraction_leaf&#39;: 0.0985158315819386, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 56, &#39;min_impurity_decrease&#39;: 0.34633650446098513, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:36,691] Trial 87 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 188, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 24, &#39;min_samples_split&#39;: 0.10641978731899052, &#39;min_samples_leaf&#39;: 0.2331406361368431, &#39;min_weight_fraction_leaf&#39;: 0.0594141903338818, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 52, &#39;min_impurity_decrease&#39;: 0.38503925310189985, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:35:38,067] Trial 88 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 62, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 9, &#39;min_samples_split&#39;: 0.2267056006047116, &#39;min_samples_leaf&#39;: 0.21412640924888954, &#39;min_weight_fraction_leaf&#39;: 0.1327240560935928, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 31, &#39;min_impurity_decrease&#39;: 0.42788635637841255, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:40,217] Trial 89 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 99, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 0.37371853245534953, &#39;min_samples_leaf&#39;: 0.19677639576983158, &#39;min_weight_fraction_leaf&#39;: 0.1738930043659333, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 70, &#39;min_impurity_decrease&#39;: 0.3083611762572587, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:41,257] Trial 90 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 47, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 27, &#39;min_samples_split&#39;: 0.1942891704386964, &#39;min_samples_leaf&#39;: 0.30146925985367246, &#39;min_weight_fraction_leaf&#39;: 0.13902409041406266, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 43, &#39;min_impurity_decrease&#39;: 0.5066925674128544, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:35:42,017] Trial 91 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 31, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 21, &#39;min_samples_split&#39;: 0.32776542629125716, &#39;min_samples_leaf&#39;: 0.2489255809100429, &#39;min_weight_fraction_leaf&#39;: 0.12340052321397474, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 62, &#39;min_impurity_decrease&#39;: 0.6469757197651401, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:35:43,635] Trial 92 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 79, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 13, &#39;min_samples_split&#39;: 0.5629703277043281, &#39;min_samples_leaf&#39;: 0.22329530661264596, &#39;min_weight_fraction_leaf&#39;: 0.15660171440379939, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 26, &#39;min_impurity_decrease&#39;: 0.6078593475483344, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:35:44,873] Trial 93 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 57, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 22, &#39;min_samples_split&#39;: 0.43793812376821467, &#39;min_samples_leaf&#39;: 0.28224107682478466, &#39;min_weight_fraction_leaf&#39;: 0.22229093553072915, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 79, &#39;min_impurity_decrease&#39;: 0.7070157214870835, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:35:45,879] Trial 94 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 41, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 0.2858601778249793, &#39;min_samples_leaf&#39;: 0.23394395338928067, &#39;min_weight_fraction_leaf&#39;: 0.10162542304507106, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 48, &#39;min_impurity_decrease&#39;: 0.47287883474828885, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:35:47,077] Trial 95 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 50, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 0.41092538548414304, &#39;min_samples_leaf&#39;: 0.21384277583984276, &#39;min_weight_fraction_leaf&#39;: 0.1814199961864852, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 7, &#39;min_impurity_decrease&#39;: 0.6765927740113548, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:35:47,792] Trial 96 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 26, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 4, &#39;min_samples_split&#39;: 0.45992308971264956, &#39;min_samples_leaf&#39;: 0.26604208637687504, &#39;min_weight_fraction_leaf&#39;: 0.20956224534072154, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 8, &#39;min_impurity_decrease&#39;: 0.5945322994102515, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:50,649] Trial 97 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 150, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 32, &#39;min_samples_split&#39;: 0.49812999175655337, &#39;min_samples_leaf&#39;: 0.3123747801777487, &#39;min_weight_fraction_leaf&#39;: 0.2373690721894015, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 59, &#39;min_impurity_decrease&#39;: 0.7562871951640474, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
[I 2023-10-14 16:35:51,537] Trial 98 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 35, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 0.2609162085209485, &#39;min_samples_leaf&#39;: 0.25409809317277615, &#39;min_weight_fraction_leaf&#39;: 0.015214961792126926, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;max_leaf_nodes&#39;: 11, &#39;min_impurity_decrease&#39;: 0.6343875625448447, &#39;bootstrap&#39;: True}. Best is trial 0 with value: 0.49965.
/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features=&#39;auto&#39;` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=&#39;sqrt&#39;` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
[I 2023-10-14 16:35:53,938] Trial 99 finished with value: 0.49965 and parameters: {&#39;n_estimators&#39;: 119, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 8, &#39;min_samples_split&#39;: 0.3789099594329366, &#39;min_samples_leaf&#39;: 0.24012583147486832, &#39;min_weight_fraction_leaf&#39;: 0.16817846923807292, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: 31, &#39;min_impurity_decrease&#39;: 0.698329823239304, &#39;bootstrap&#39;: False}. Best is trial 0 with value: 0.49965.
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Number of finished trials: 100
Best trial: {&#39;n_estimators&#39;: 158, &#39;criterion&#39;: &#39;log_loss&#39;, &#39;max_depth&#39;: 17, &#39;min_samples_split&#39;: 0.3880243171996576, &#39;min_samples_leaf&#39;: 0.4247838122008928, &#39;min_weight_fraction_leaf&#39;: 0.170287707952885, &#39;max_features&#39;: &#39;log2&#39;, &#39;max_leaf_nodes&#39;: 7, &#39;min_impurity_decrease&#39;: 0.6467542020296969, &#39;bootstrap&#39;: True}
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="44"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T16:35:53.960771Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T16:35:53.959802Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T16:35:53.966652Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T16:35:53.965526Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T16:35:53.960735Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb179"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the dimensions and the type of the feature matrix</span></span>
<span id="cb179-2"><a href="#cb179-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X.shape, <span class="bu">type</span>(X))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>(100000, 1048576) &lt;class &#39;scipy.sparse.csr.csr_matrix&#39;&gt;
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="45"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-10-14T16:35:53.968849Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-10-14T16:35:53.968187Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-10-14T16:35:57.028483Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-10-14T16:35:57.027503Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-10-14T16:35:53.968813Z&quot;}"
data-trusted="true">
<div class="sourceCode" id="cb181"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Assumez que best_params est le dictionnaire des meilleurs paramtres retourns par Optuna</span></span>
<span id="cb181-2"><a href="#cb181-2" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> study.best_trial.params</span>
<span id="cb181-3"><a href="#cb181-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-4"><a href="#cb181-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Crez un nouveau classificateur Random Forest avec ces paramtres</span></span>
<span id="cb181-5"><a href="#cb181-5" aria-hidden="true" tabindex="-1"></a>final_classifier <span class="op">=</span> RandomForestClassifier(<span class="op">**</span>best_params, random_state<span class="op">=</span><span class="dv">50</span>, verbose<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb181-6"><a href="#cb181-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-7"><a href="#cb181-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Entranez le classificateur sur l&#39;ensemble d&#39;entranement</span></span>
<span id="cb181-8"><a href="#cb181-8" aria-hidden="true" tabindex="-1"></a>final_classifier.fit(X_train, y_train)</span>
<span id="cb181-9"><a href="#cb181-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-10"><a href="#cb181-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Faites des prdictions sur l&#39;ensemble de test</span></span>
<span id="cb181-11"><a href="#cb181-11" aria-hidden="true" tabindex="-1"></a>final_predictions <span class="op">=</span> final_classifier.predict(X_test)</span>
<span id="cb181-12"><a href="#cb181-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-13"><a href="#cb181-13" aria-hidden="true" tabindex="-1"></a><span class="co"># valuez la performance du classificateur</span></span>
<span id="cb181-14"><a href="#cb181-14" aria-hidden="true" tabindex="-1"></a>final_accuracy <span class="op">=</span> accuracy_score(y_test, final_predictions)</span>
<span id="cb181-15"><a href="#cb181-15" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, final_predictions)</span>
<span id="cb181-16"><a href="#cb181-16" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y_test, final_predictions)</span>
<span id="cb181-17"><a href="#cb181-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-18"><a href="#cb181-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Print les mtriques d&#39;valuation</span></span>
<span id="cb181-19"><a href="#cb181-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Final Accuracy: </span><span class="sc">{</span>final_accuracy<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb181-20"><a href="#cb181-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Confusion Matrix:</span><span class="ch">\n</span><span class="sc">{</span>conf_matrix<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb181-21"><a href="#cb181-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Classification Report:</span><span class="ch">\n</span><span class="sc">{</span>class_report<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.4s
[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.5s
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Final Accuracy: 0.49965
Confusion Matrix:
[[ 9993     0]
 [10007     0]]
Classification Report:
              precision    recall  f1-score   support

           0       0.50      1.00      0.67      9993
           1       0.00      0.00      0.00     10007

    accuracy                           0.50     20000
   macro avg       0.25      0.50      0.33     20000
weighted avg       0.25      0.50      0.33     20000

</code></pre>
</div>
<div class="output stream stderr">
<pre><code>/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Even with few data, Random Forest is not able to discern anything
from it.</p>
<p>This is probably due to the fact that the data is too complex for
this model.</p>
<p>We also tried RandomForest on more data but it didn't yield better
results, it also was very costly in terms of memory and time.</p>
</div>
<div class="cell markdown">
<hr />
</div>
<section id="multi-layer-perceptron-mlp" class="cell markdown">
<h2>Multi Layer Perceptron (MLP)</h2>
<p>Multi Layer Perceptron (MLP) is a class of feedforward artificial
neural network. By flattening the DNA sequences and employing a suitable
encoding scheme, MLPs can be trained to differentiate between human and
gorilla DNA based on the input features.</p>
<p>The idea being that the MLP will be able to learn the patterns in the
data and classify the sequences accordingly.</p>
<p>This was also a more general approach to the problem as MLPs are able
to learn from any kind of data.</p>
</section>
<div class="cell code">
<div class="sourceCode" id="cb185"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install sklearn<span class="op">-</span>genetic<span class="op">-</span>opt scikit<span class="op">-</span>learn tqdm matplotlib numpy pandas</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple
Requirement already satisfied: sklearn-genetic-opt in /home/arnaudb/Documents/EPITA/ING3/ml-bio/.venv/lib/python3.11/site-packages (0.10.1)
Requirement already satisfied: scikit-learn&gt;=1.1.0 in /home/arnaudb/Documents/EPITA/ING3/ml-bio/.venv/lib/python3.11/site-packages (from sklearn-genetic-opt) (1.3.0)
Requirement already satisfied: numpy&gt;=1.19.0 in /home/arnaudb/Documents/EPITA/ING3/ml-bio/.venv/lib/python3.11/site-packages (from sklearn-genetic-opt) (1.24.3)
Requirement already satisfied: deap&gt;=1.3.3 in /home/arnaudb/Documents/EPITA/ING3/ml-bio/.venv/lib/python3.11/site-packages (from sklearn-genetic-opt) (1.4.1)
Requirement already satisfied: tqdm&gt;=4.61.1 in /home/arnaudb/Documents/EPITA/ING3/ml-bio/.venv/lib/python3.11/site-packages (from sklearn-genetic-opt) (4.66.1)
Requirement already satisfied: scipy&gt;=1.5.0 in /home/arnaudb/Documents/EPITA/ING3/ml-bio/.venv/lib/python3.11/site-packages (from scikit-learn&gt;=1.1.0-&gt;sklearn-genetic-opt) (1.9.3)
Requirement already satisfied: joblib&gt;=1.1.1 in /home/arnaudb/Documents/EPITA/ING3/ml-bio/.venv/lib/python3.11/site-packages (from scikit-learn&gt;=1.1.0-&gt;sklearn-genetic-opt) (1.3.2)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /home/arnaudb/Documents/EPITA/ING3/ml-bio/.venv/lib/python3.11/site-packages (from scikit-learn&gt;=1.1.0-&gt;sklearn-genetic-opt) (3.2.0)
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb187"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb187-2"><a href="#cb187-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn_genetic <span class="im">import</span> GASearchCV</span>
<span id="cb187-3"><a href="#cb187-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn_genetic.space <span class="im">import</span> Categorical, Integer, Continuous</span>
<span id="cb187-4"><a href="#cb187-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, StratifiedKFold</span>
<span id="cb187-5"><a href="#cb187-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPClassifier</span>
<span id="cb187-6"><a href="#cb187-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb187-7"><a href="#cb187-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span></code></pre></div>
<div class="output stream stderr">
<pre><code>2023-10-14 18:11:02.666477: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-14 18:11:03.900152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb189"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb189-2"><a href="#cb189-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;tol&quot;</span>: Continuous(<span class="fl">1e-2</span>, <span class="fl">1e10</span>, distribution<span class="op">=</span><span class="st">&quot;log-uniform&quot;</span>),</span>
<span id="cb189-3"><a href="#cb189-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;learning_rate&quot;</span>: Categorical([<span class="st">&quot;constant&quot;</span>, <span class="st">&quot;adaptive&quot;</span>]),</span>
<span id="cb189-4"><a href="#cb189-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;hidden_layer_sizes&quot;</span>: Integer(<span class="dv">10</span>, <span class="dv">1000</span>),</span>
<span id="cb189-5"><a href="#cb189-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;max_iter&quot;</span>: Integer(<span class="dv">100</span>, <span class="dv">1000</span>),</span>
<span id="cb189-6"><a href="#cb189-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb190"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb190-1"><a href="#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The base classifier to tune</span></span>
<span id="cb190-2"><a href="#cb190-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MLPClassifier(</span>
<span id="cb190-3"><a href="#cb190-3" aria-hidden="true" tabindex="-1"></a>    early_stopping<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span>random_state, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>, solver<span class="op">=</span><span class="st">&quot;adam&quot;</span></span>
<span id="cb190-4"><a href="#cb190-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb190-5"><a href="#cb190-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-6"><a href="#cb190-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Our cross-validation strategy (it could be just an int)</span></span>
<span id="cb190-7"><a href="#cb190-7" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb190-8"><a href="#cb190-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-9"><a href="#cb190-9" aria-hidden="true" tabindex="-1"></a><span class="co"># The main class from sklearn-genetic-opt</span></span>
<span id="cb190-10"><a href="#cb190-10" aria-hidden="true" tabindex="-1"></a>evolved_estimator <span class="op">=</span> GASearchCV(</span>
<span id="cb190-11"><a href="#cb190-11" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>clf,</span>
<span id="cb190-12"><a href="#cb190-12" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv,</span>
<span id="cb190-13"><a href="#cb190-13" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">&quot;accuracy&quot;</span>,</span>
<span id="cb190-14"><a href="#cb190-14" aria-hidden="true" tabindex="-1"></a>    param_grid<span class="op">=</span>param_grid,</span>
<span id="cb190-15"><a href="#cb190-15" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb190-16"><a href="#cb190-16" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb190-17"><a href="#cb190-17" aria-hidden="true" tabindex="-1"></a>    population_size<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb190-18"><a href="#cb190-18" aria-hidden="true" tabindex="-1"></a>    generations<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb190-19"><a href="#cb190-19" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb191"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb191-2"><a href="#cb191-2" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size<span class="op">=</span><span class="dv">2048</span>, shuffle<span class="op">=</span><span class="va">True</span></span>
<span id="cb191-3"><a href="#cb191-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb191-4"><a href="#cb191-4" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb191-5"><a href="#cb191-5" aria-hidden="true" tabindex="-1"></a>    test_dataset, batch_size<span class="op">=</span><span class="dv">2048</span>, shuffle<span class="op">=</span><span class="va">True</span></span>
<span id="cb191-6"><a href="#cb191-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb191-7"><a href="#cb191-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb191-8"><a href="#cb191-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> mini_batch <span class="kw">in</span> tqdm(train_dataloader, total<span class="op">=</span><span class="bu">len</span>(train_dataloader)):</span>
<span id="cb191-9"><a href="#cb191-9" aria-hidden="true" tabindex="-1"></a>    X_train, y_train <span class="op">=</span> mini_batch</span>
<span id="cb191-10"><a href="#cb191-10" aria-hidden="true" tabindex="-1"></a>    X_train <span class="op">=</span> X_train.reshape(X_train.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb191-11"><a href="#cb191-11" aria-hidden="true" tabindex="-1"></a>    evolved_estimator.fit(X_train, y_train)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/40 [00:00&lt;?, ?it/s]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.498685	0.0082103  	0.513212   	0.484381   
1  	40    	0.508744	0.00770474 	0.521487   	0.492679   
2  	40    	0.512236	0.00521099 	0.521487   	0.499503   
3  	40    	0.513773	0.00557615 	0.521487   	0.502442   
4  	40    	0.515701	0.00514096 	0.521481   	0.502439   
5  	40    	0.514528	0.00649357 	0.522457   	0.503425   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  2%|         | 1/40 [03:56&lt;2:33:57, 236.85s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.504443	0.0105353  	0.528329   	0.477536   
1  	40    	0.510058	0.0074114  	0.524894   	0.494137   
2  	40    	0.51582 	0.00489588 	0.52345    	0.506337   
3  	40    	0.52283 	0.00655065 	0.533696   	0.514644   
4  	40    	0.52178 	0.00834658 	0.533696   	0.504895   
5  	40    	0.521216	0.00918099 	0.533696   	0.503408   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  5%|         | 2/40 [09:35&lt;3:07:53, 296.67s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.511769	0.0110913  	0.527338   	0.490735   
1  	40    	0.520484	0.0095636  	0.540037   	0.504396   
2  	40    	0.521313	0.0100477  	0.540037   	0.505371   
3  	40    	0.523365	0.00795575 	0.540032   	0.507331   
4  	40    	0.524315	0.00628912 	0.536626   	0.51612    
5  	40    	0.523923	0.00576991 	0.536626   	0.515125   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  8%|         | 3/40 [15:08&lt;3:13:08, 313.20s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.510866	0.00778352 	0.528319   	0.497563   
1  	40    	0.519874	0.00729202 	0.533208   	0.508294   
2  	40    	0.523952	0.00518228 	0.533208   	0.514653   
3  	40    	0.524148	0.00623212 	0.533208   	0.507809   
4  	40    	0.526346	0.00607857 	0.533208   	0.509771   
5  	40    	0.526837	0.00613137 	0.531738   	0.513669   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 10%|         | 4/40 [20:09&lt;3:05:08, 308.56s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.518748	0.00606619 	0.532716   	0.510741   
1  	40    	0.52441 	0.00440474 	0.532716   	0.515618   
2  	40    	0.524632	0.00396133 	0.532716   	0.518062   
3  	40    	0.526071	0.0056131  	0.535156   	0.513181   
4  	40    	0.527099	0.00693648 	0.535156   	0.513198   
5  	40    	0.528369	0.00704861 	0.535161   	0.515626   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 12%|        | 5/40 [25:43&lt;3:05:19, 317.71s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.488429	0.0123596  	0.524899   	0.473633   
1  	40    	0.501415	0.00764229 	0.524899   	0.488774   
2  	40    	0.507054	0.00886358 	0.524899   	0.49561    
3  	40    	0.514676	0.00759128 	0.524899   	0.50049    
4  	40    	0.514382	0.0065182  	0.524899   	0.50049    
5  	40    	0.51526 	0.00681495 	0.524899   	0.50342    
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 15%|        | 6/40 [31:33&lt;3:06:12, 328.61s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.497315	0.00941182 	0.515134   	0.479986   
1  	40    	0.505174	0.00665919 	0.515134   	0.493175   
2  	40    	0.50896 	0.00517129 	0.515134   	0.501466   
3  	40    	0.509933	0.00620115 	0.516598   	0.498527   
4  	40    	0.512816	0.00635069 	0.522457   	0.498527   
5  	40    	0.515232	0.00597385 	0.522457   	0.501473   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 18%|        | 7/40 [37:20&lt;3:04:08, 334.80s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.506323	0.00943555 	0.520993   	0.491202   
1  	40    	0.515598	0.0068931  	0.522457   	0.500979   
2  	40    	0.521387	0.00664265 	0.534192   	0.507317   
3  	40    	0.524294	0.00631432 	0.534192   	0.510744   
4  	40    	0.52603 	0.00673795 	0.538094   	0.514171   
5  	40    	0.522049	0.00742426 	0.534192   	0.510738   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 20%|        | 8/40 [43:09&lt;3:00:53, 339.16s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.502321	0.00798495 	0.516607   	0.489743   
1  	40    	0.508302	0.00589714 	0.522956   	0.499504   
2  	40    	0.512357	0.00695804 	0.526358   	0.502933   
3  	40    	0.516774	0.00708559 	0.526358   	0.504878   
4  	40    	0.51741 	0.00407694 	0.523438   	0.509762   
5  	40    	0.519339	0.0047173  	0.523438   	0.504386   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 22%|       | 9/40 [49:05&lt;2:58:01, 344.57s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.505079	0.0104477  	0.5288     	0.486813   
1  	40    	0.513209	0.00743001 	0.5288     	0.499015   
2  	40    	0.515917	0.00590084 	0.528314   	0.500485   
3  	40    	0.516408	0.00664467 	0.528314   	0.504885   
4  	40    	0.518018	0.00908636 	0.532232   	0.503415   
5  	40    	0.519215	0.00783945 	0.532232   	0.497072   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 25%|       | 10/40 [53:40&lt;2:41:27, 322.93s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness	fitness_std	fitness_max	fitness_min
0  	20    	0.50276	0.00993498 	0.526864   	0.485352   
1  	40    	0.51062	0.00793514 	0.526864   	0.498532   
2  	40    	0.515941	0.00565127 	0.526864   	0.500005   
3  	40    	0.519702	0.0051276  	0.528812   	0.513672   
4  	40    	0.51738 	0.00695647 	0.528812   	0.503902   
5  	40    	0.518969	0.00440375 	0.528812   	0.512701   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 28%|       | 11/40 [59:10&lt;2:37:05, 325.02s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.502465	0.00825292 	0.521971   	0.487309   
1  	40    	0.509449	0.00717156 	0.521971   	0.498541   
2  	40    	0.514062	0.00663982 	0.526356   	0.504391   
3  	40    	0.513237	0.00737696 	0.521971   	0.493654   
4  	40    	0.517408	0.00529564 	0.523439   	0.50635    
5  	40    	0.516137	0.00650752 	0.524917   	0.504876   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 30%|       | 12/40 [1:04:17&lt;2:29:12, 319.72s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.491457	0.0117886  	0.517093   	0.468269   
1  	40    	0.502055	0.00826984 	0.517093   	0.486803   
2  	40    	0.505374	0.00393375 	0.51319    	0.500015   
3  	40    	0.507494	0.00659232 	0.518559   	0.493171   
4  	40    	0.509836	0.00414478 	0.514648   	0.500974   
5  	40    	0.509567	0.00478609 	0.515131   	0.500974   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 32%|      | 13/40 [1:08:10&lt;2:11:58, 293.29s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.494432	0.008656   	0.508788   	0.471199   
1  	40    	0.504051	0.00744926 	0.514165   	0.4839     
2  	40    	0.509057	0.0056038  	0.514645   	0.491207   
3  	40    	0.510867	0.00531692 	0.516613   	0.495125   
4  	40    	0.510256	0.00530404 	0.519039   	0.49657    
5  	40    	0.509499	0.00566244 	0.519039   	0.496091   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 35%|      | 14/40 [1:12:05&lt;1:59:26, 275.63s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.511351	0.0112474  	0.536132   	0.488286   
1  	40    	0.519359	0.0065757  	0.533209   	0.505379   
2  	40    	0.522752	0.00627402 	0.531735   	0.503896   
3  	40    	0.527125	0.00594824 	0.535164   	0.514162   
4  	40    	0.524636	0.0081998  	0.53418    	0.509765   
5  	40    	0.526   	0.00843065 	0.53418    	0.505363   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 38%|      | 15/40 [1:17:00&lt;1:57:18, 281.54s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.502417	0.00848111 	0.515617   	0.484858   
1  	40    	0.508395	0.00694687 	0.526361   	0.498535   
2  	40    	0.513111	0.00508521 	0.518081   	0.500988   
3  	40    	0.514261	0.00667442 	0.532222   	0.5        
4  	40    	0.516799	0.00522983 	0.524419   	0.50829    
5  	40    	0.517578	0.00609527 	0.530273   	0.50829    
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 40%|      | 16/40 [1:22:02&lt;1:55:02, 287.62s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.500144	0.00971477 	0.518547   	0.481942   
1  	40    	0.508644	0.0046381  	0.518063   	0.501954   
2  	40    	0.513453	0.00583702 	0.52197    	0.502929   
3  	40    	0.516431	0.00567453 	0.525389   	0.506345   
4  	40    	0.516674	0.00752583 	0.525389   	0.499509   
5  	40    	0.515187	0.00576505 	0.523921   	0.500495   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 42%|     | 17/40 [1:27:02&lt;1:51:44, 291.48s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.500097	0.0124277  	0.521474   	0.469729   
1  	40    	0.509353	0.00615315 	0.521474   	0.497574   
2  	40    	0.508033	0.00735152 	0.518571   	0.493643   
3  	40    	0.511014	0.00674368 	0.520508   	0.495129   
4  	40    	0.513208	0.0047366  	0.520508   	0.499008   
5  	40    	0.511793	0.0057545  	0.519047   	0.493152   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 45%|     | 18/40 [1:32:21&lt;1:49:57, 299.88s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.489575	0.00884876 	0.505377   	0.468745   
1  	40    	0.50054 	0.00651878 	0.509762   	0.486819   
2  	40    	0.501223	0.00711498 	0.509762   	0.485847   
3  	40    	0.506203	0.00492371 	0.520517   	0.498538   
4  	40    	0.504027	0.00596764 	0.520517   	0.493642   
5  	40    	0.503201	0.00653425 	0.520517   	0.489743   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 48%|     | 19/40 [1:37:41&lt;1:46:59, 305.69s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.509399	0.0108075  	0.522955   	0.479989   
1  	40    	0.514647	0.00498035 	0.521961   	0.500497   
2  	40    	0.516233	0.00444525 	0.521961   	0.506851   
3  	40    	0.520482	0.00391661 	0.525873   	0.509291   
4  	40    	0.520483	0.00570502 	0.528319   	0.501947   
5  	40    	0.521875	0.00449723 	0.528319   	0.511714   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 50%|     | 20/40 [1:41:37&lt;1:34:59, 284.97s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.496926	0.00772282 	0.509775   	0.479486   
1  	40    	0.504958	0.00310056 	0.509281   	0.499034   
2  	40    	0.507865	0.00546284 	0.519536   	0.496588   
3  	40    	0.506398	0.00633475 	0.519536   	0.492679   
4  	40    	0.509887	0.00599174 	0.519536   	0.498529   
5  	40    	0.508545	0.00533256 	0.516601   	0.496101   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 52%|    | 21/40 [1:46:17&lt;1:29:45, 283.46s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.498875	0.00746068 	0.512691   	0.486813   
1  	40    	0.509765	0.00550762 	0.517593   	0.497555   
2  	40    	0.510962	0.00531089 	0.517593   	0.500013   
3  	40    	0.515235	0.00360388 	0.521477   	0.504401   
4  	40    	0.516554	0.00632141 	0.525881   	0.503401   
5  	40    	0.515094	0.00501734 	0.525881   	0.505874   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 55%|    | 22/40 [1:50:11&lt;1:20:33, 268.51s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.490187	0.0082752  	0.504895   	0.470706   
1  	40    	0.498148	0.00683988 	0.517084   	0.486828   
2  	40    	0.504124	0.00542426 	0.517084   	0.49317    
3  	40    	0.506053	0.00737289 	0.517084   	0.49122    
4  	40    	0.505957	0.00709233 	0.51759    	0.491224   
5  	40    	0.507543	0.00594747 	0.517084   	0.494635   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 57%|    | 23/40 [1:54:36&lt;1:15:49, 267.59s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.495482	0.0085394  	0.515633   	0.478019   
1  	40    	0.505543	0.00849456 	0.515633   	0.488288   
2  	40    	0.509642	0.00541962 	0.520015   	0.500972   
3  	40    	0.51262 	0.0062705  	0.52343    	0.497558   
4  	40    	0.515991	0.00599206 	0.530772   	0.508789   
5  	40    	0.51609 	0.00924722 	0.530772   	0.49121    
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 60%|    | 24/40 [1:59:57&lt;1:15:35, 283.47s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.497559	0.00861196 	0.512215   	0.48194    
1  	40    	0.50188 	0.00740743 	0.517092   	0.487305   
2  	40    	0.504224	0.00746989 	0.523423   	0.493665   
3  	40    	0.507448	0.00787835 	0.523423   	0.495124   
4  	40    	0.507422	0.00609355 	0.517092   	0.491699   
5  	40    	0.50957 	0.00613409 	0.521978   	0.495114   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 62%|   | 25/40 [2:05:19&lt;1:13:46, 295.10s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.497365	0.00829461 	0.51025    	0.481449   
1  	40    	0.509179	0.00755261 	0.525881   	0.494136   
2  	40    	0.5094  	0.00656768 	0.517099   	0.49415    
3  	40    	0.512671	0.00936658 	0.530283   	0.496584   
4  	40    	0.515822	0.00862684 	0.530283   	0.496584   
5  	40    	0.515579	0.00738979 	0.530283   	0.501471   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 65%|   | 26/40 [2:10:51&lt;1:11:26, 306.19s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness	fitness_std	fitness_max	fitness_min
0  	20    	0.50681	0.0120467  	0.524909   	0.476565   
1  	40    	0.515967	0.00769728 	0.532234   	0.503914   
2  	40    	0.519019	0.00539151 	0.532234   	0.510256   
3  	40    	0.523903	0.00608437 	0.532234   	0.511707   
4  	40    	0.526222	0.00283987 	0.532234   	0.520023   
5  	40    	0.52659 	0.00540702 	0.543459   	0.520023   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 68%|   | 27/40 [2:14:20&lt;1:00:01, 277.01s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness	fitness_std	fitness_max	fitness_min
0  	20    	0.50371	0.00771469 	0.520036   	0.488292   
1  	40    	0.509181	0.00765854 	0.529291   	0.492186   
2  	40    	0.513258	0.00453093 	0.523925   	0.508308   
3  	40    	0.513405	0.00544905 	0.523925   	0.503423   
4  	40    	0.513132	0.00403571 	0.523925   	0.507311   
5  	40    	0.513451	0.00549948 	0.523925   	0.50488    
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 70%|   | 28/40 [2:17:51&lt;51:27, 257.27s/it]  </code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.494435	0.00751497 	0.504879   	0.479495   
1  	40    	0.499339	0.00677811 	0.509765   	0.485847   
2  	40    	0.50464 	0.00498903 	0.512703   	0.493656   
3  	40    	0.507471	0.00476011 	0.51465    	0.499031   
4  	40    	0.509935	0.00412223 	0.51465    	0.500494   
5  	40    	0.513012	0.00411762 	0.521002   	0.50586    
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 72%|  | 29/40 [2:22:45&lt;49:09, 268.12s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.498585	0.0072585  	0.512201   	0.485832   
1  	40    	0.508227	0.00610368 	0.516105   	0.496578   
2  	40    	0.506469	0.0053968  	0.516105   	0.499517   
3  	40    	0.507321	0.00593425 	0.516105   	0.494614   
4  	40    	0.507516	0.00650924 	0.516105   	0.496084   
5  	40    	0.509616	0.00746072 	0.517093   	0.493657   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 75%|  | 30/40 [2:27:35&lt;45:49, 274.90s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.497656	0.00993836 	0.52148    	0.479495   
1  	40    	0.512276	0.00737103 	0.522457   	0.497066   
2  	40    	0.514892	0.00614348 	0.522457   	0.504405   
3  	40    	0.511792	0.00788419 	0.522457   	0.496089   
4  	40    	0.512109	0.00797683 	0.521964   	0.495117   
5  	40    	0.51245 	0.00550416 	0.521964   	0.499993   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 78%|  | 31/40 [2:34:04&lt;46:20, 308.91s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.515917	0.0106752  	0.533702   	0.496584   
1  	40    	0.52771 	0.00617609 	0.536134   	0.514655   
2  	40    	0.527832	0.00659342 	0.536134   	0.511717   
3  	40    	0.529053	0.00688069 	0.539065   	0.510258   
4  	40    	0.526172	0.00731504 	0.539065   	0.511226   
5  	40    	0.524319	0.0092081  	0.544935   	0.506844   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 80%|  | 32/40 [2:38:23&lt;39:13, 294.16s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.500877	0.00958976 	0.517577   	0.484365   
1  	40    	0.509691	0.00687076 	0.517577   	0.494629   
2  	40    	0.511961	0.00659911 	0.520505   	0.497074   
3  	40    	0.510717	0.00587437 	0.51757    	0.500975   
4  	40    	0.511182	0.0048996  	0.51757    	0.502444   
5  	40    	0.511549	0.00639208 	0.51757    	0.492185   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 82%| | 33/40 [2:44:18&lt;36:26, 312.40s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.506276	0.00697605 	0.517578   	0.484876   
1  	40    	0.515553	0.00510874 	0.52685    	0.506347   
2  	40    	0.518261	0.00584945 	0.529305   	0.508295   
3  	40    	0.52019 	0.0055383  	0.529305   	0.509764   
4  	40    	0.520776	0.00649917 	0.529305   	0.508302   
5  	40    	0.520996	0.00658871 	0.529305   	0.508302   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 85%| | 34/40 [2:48:32&lt;29:27, 294.65s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.507567	0.00971542 	0.531739   	0.493162   
1  	40    	0.512499	0.00826331 	0.531739   	0.496582   
2  	40    	0.517406	0.00683463 	0.531739   	0.505864   
3  	40    	0.519725	0.00571492 	0.531739   	0.503412   
4  	40    	0.519898	0.00648072 	0.531739   	0.509773   
5  	40    	0.523366	0.0052854  	0.531739   	0.505367   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 88%| | 35/40 [2:53:05&lt;24:01, 288.26s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.481421	0.00923024 	0.496099   	0.461422   
1  	40    	0.492943	0.00788276 	0.506349   	0.470697   
2  	40    	0.494555	0.00654016 	0.506349   	0.47509    
3  	40    	0.497656	0.00460228 	0.505371   	0.489752   
4  	40    	0.50078 	0.00506398 	0.507814   	0.492189   
5  	40    	0.502466	0.00681238 	0.515632   	0.487798   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 90%| | 36/40 [2:56:37&lt;17:42, 265.52s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.497288	0.00934876 	0.514163   	0.480958   
1  	40    	0.506344	0.00812977 	0.520504   	0.490233   
2  	40    	0.512033	0.00620457 	0.520504   	0.49805    
3  	40    	0.512962	0.00652273 	0.523905   	0.500485   
4  	40    	0.509231	0.0074831  	0.520504   	0.494155   
5  	40    	0.511915	0.00537543 	0.520504   	0.500485   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 92%|| 37/40 [3:01:31&lt;13:41, 273.78s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.512012	0.0101134  	0.531255   	0.489265   
1  	40    	0.519165	0.00834789 	0.531255   	0.504397   
2  	40    	0.525026	0.00574818 	0.534189   	0.515618   
3  	40    	0.527979	0.00325553 	0.533208   	0.520994   
4  	40    	0.528882	0.00465155 	0.533208   	0.511716   
5  	40    	0.526413	0.0045601  	0.531256   	0.516113   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 95%|| 38/40 [3:06:26&lt;09:20, 280.26s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness	fitness_std	fitness_max	fitness_min
0  	20    	0.51272	0.00759689 	0.529299   	0.501467   
1  	40    	0.524242	0.00829995 	0.53614    	0.510747   
2  	40    	0.52307 	0.00736938 	0.53614    	0.50977    
3  	40    	0.525783	0.00862631 	0.538087   	0.506343   
4  	40    	0.529223	0.00856587 	0.54395    	0.511717   
5  	40    	0.528688	0.00558676 	0.538087   	0.515625   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code> 98%|| 39/40 [3:11:12&lt;04:41, 281.86s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>gen	nevals	fitness 	fitness_std	fitness_max	fitness_min
0  	20    	0.479983	0.031994   	0.52344    	0.406238   
1  	40    	0.512191	0.0215121  	0.563677   	0.453304   
2  	40    	0.527021	0.0156638  	0.562385   	0.492433   
3  	40    	0.540799	0.0301578  	0.609819   	0.507752   
4  	40    	0.552925	0.0309622  	0.609819   	0.507752   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 40/40 [3:11:46&lt;00:00, 287.66s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>5  	40    	0.546899	0.0229995  	0.58564    	0.507383   
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb275"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb275-1"><a href="#cb275-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Score</span></span>
<span id="cb275-2"><a href="#cb275-2" aria-hidden="true" tabindex="-1"></a>mean_acc <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb275-3"><a href="#cb275-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> mini_batch <span class="kw">in</span> tqdm(test_dataloader, total<span class="op">=</span><span class="bu">len</span>(test_dataloader)):</span>
<span id="cb275-4"><a href="#cb275-4" aria-hidden="true" tabindex="-1"></a>    X_test, y_test <span class="op">=</span> mini_batch</span>
<span id="cb275-5"><a href="#cb275-5" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> X_test.reshape(X_test.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb275-6"><a href="#cb275-6" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> evolved_estimator.predict(X_test)</span>
<span id="cb275-7"><a href="#cb275-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb275-8"><a href="#cb275-8" aria-hidden="true" tabindex="-1"></a>    mean_acc <span class="op">+=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb275-9"><a href="#cb275-9" aria-hidden="true" tabindex="-1"></a>mean_acc <span class="op">/=</span> <span class="bu">len</span>(test_dataloader)</span>
<span id="cb275-10"><a href="#cb275-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb275-11"><a href="#cb275-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Mean accuracy per mini_batch: </span><span class="sc">{</span>mean_acc<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>100%|| 10/10 [00:03&lt;00:00,  2.75it/s]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Mean accuracy per mini_batch: 0.495
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>MLPs didn't yield any good results.</p>
<p>Some informations are of note however:</p>
<ul>
<li>The model was able to learn something from the data as the accuracy
was increasing with the number of epochs, however generalization was not
good as we lost all progress eveytime we changed the training
mini-batch.</li>
<li>Mini-batching seems (obviously) the better approach as it is faster
and more efficient, however it reduces our generalization
capabilities.</li>
<li>MLPs might be a good approach for this problem but it needs much
more computational power and time to train.</li>
</ul>
</div>
<section id="conclusion" class="cell markdown">
<h2>Conclusion</h2>
<p>We tried several approaches to obtain better results than the
competitors of the competition. However, we were not able to do so.</p>
<p>Whatever the approach, the accuracy was always around 50% which is
the accuracy of random guessing.</p>
<p>We managed to reproduce the results of the best competitors but we
were not able to improve them.</p>
<p>Among all our approaches, it seems that MLPs and CNNs are the
best.</p>
<p>Also, some more research is needed to find a relevant way to encode
the data for the models to be able to learn from it.</p>
<p>All in all, this was a very interesting project as it was a very hard
problem to solve and we learned a lot from it.</p>
</section>
</body>
</html>
